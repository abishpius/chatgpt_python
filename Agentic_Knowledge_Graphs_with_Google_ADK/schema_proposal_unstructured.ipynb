{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np0plMPXRvoq"
   },
   "source": [
    "# Lesson 7 - Schema Proposal for Unstructured Data\n",
    "\n",
    "In this lesson, you will design agents that propose how to extract information from unstructured data.\n",
    "\n",
    "You'll learn:\n",
    "- how \"named entity recognition\" can be used to identify the people, places and things\n",
    "- how facts can be extracted as a \"triple\" of subject, predicate and object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>To access the helper.py, neo4j_for_adk.py and tools.py files :</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/entire_solution.png\" width=\"500\">\n",
    "\n",
    "Two agents to propose data extraction from unstructured data: a \"named entity recognition\" agent and a \"fact extraction\" agent:\n",
    "\n",
    "- Input: `approved_user_goal`, `approved_files`, `approved_construction_plan`\n",
    "- Output: \n",
    "    - `approved_entity_types` describing the type of entities that could be extracted from the unstructured data\n",
    "    - `approved_fact_types` describing how those entities can be related in a fact triple\n",
    "- Tools: `get_approved_user_goal`, `get_approved_files`, `get_well_known_types`, `sample_file`, `set_proposed_entities`, `get_proposed_entities`, `approve_proposed_entities`, `add_proposed_fact`, `get_proposed_facts`, `approve_proposed_facts`\n",
    "\n",
    "**Workflow**\n",
    "\n",
    "<img src=\"images/workflow.png\" width=\"500\">\n",
    "\n",
    "Named entity recognition:\n",
    "\n",
    "1. The context is initialized with an `approved_user_goal`, `approved_files` and an `approved_construction_plan`\n",
    "2. The agent analyzes unstructured data files, looking for relevant entity types \n",
    "3. The agent proposes a list of entity types, seeking user approval\n",
    "\n",
    "Fact extraction:\n",
    "\n",
    "1. The context now includes `approved_entity_types`\n",
    "2. The agent analyzes unstructured data files, looking for how those entities can be saved as fact triples\n",
    "3. The agent proposes fact types, seeking user approval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual import of needed libraries, loading of environment variables, and connection to Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 301,
    "id": "sbwxKypOSBkN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For OpenAI support\n",
    "from google.adk.tools import ToolContext\n",
    "\n",
    "# Convenience libraries for working with Neo4j inside of Google ADK\n",
    "from neo4j_for_adk import graphdb, tool_success, tool_error\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 184,
    "id": "MI_qvZJrSJuR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-C947eeg4IbKothnJLYih2xvRqZQYP', created=1756276834, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_46bff0e0c8', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Yes, I'm ready! How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=13, prompt_tokens=27, total_tokens=40, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "\n",
      "OpenAI ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT_4O)\n",
    "\n",
    "# Test LLM with a direct call\n",
    "print(llm.llm_client.completion(model=llm.model, messages=[{\"role\": \"user\", \"content\": \"Are you ready?\"}], tools=[]))\n",
    "\n",
    "print(\"\\nOpenAI ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 116
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'query_result': [{'message': 'Neo4j is Ready!'}]}\n"
     ]
    }
   ],
   "source": [
    "# Check connection to Neo4j by sending a query\n",
    "\n",
    "neo4j_is_ready = graphdb.send_query(\"RETURN 'Neo4j is Ready!' as message\")\n",
    "\n",
    "print(neo4j_is_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Named Entity Recognition (NER) Sub-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NER agent is responsible for proposing entities that could be extracted from the unstructured data files.\n",
    "\n",
    "An entity is a person, place or thing that is relevant to the user's goal.\n",
    "\n",
    "There are two general kinds of entities:\n",
    "\n",
    "1. Well-known entities: these closely correlate with nodes in the existing structured data\n",
    "   - in our example, this would be things like Products, Parts and Suppliers\n",
    "2. Discovered entities: these are entities that are not pre-defined, but are mentioned in the markdown text\n",
    "    - in the product reviews, this may may Reviewers, product complaints, or product features\n",
    "\n",
    "The general goal of the NER agent is to analyze the markdown files and propose entities that are \n",
    "relevant to the user goal of root-cause analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. Agent Instructions (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 116
   },
   "outputs": [],
   "source": [
    "ner_agent_role_and_goal = \"\"\"\n",
    "  You are a top-tier algorithm designed for analyzing text files and proposing\n",
    "  the kind of named entities that could be extracted which would be relevant \n",
    "  for a user's goal.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 371
   },
   "outputs": [],
   "source": [
    "ner_agent_hints = \"\"\"\n",
    "  Entities are people, places, things and qualities, but not quantities. \n",
    "  Your goal is to propose a list of the type of entities, not the actual instances\n",
    "  of entities.\n",
    "\n",
    "  There are two general approaches to identifying types of entities:\n",
    "  - well-known entities: these closely correlate with approved node labels in an existing graph schema\n",
    "  - discovered entities: these may not exist in the graph schema, but appear consistently in the source text\n",
    "\n",
    "  Design rules for well-known entities:\n",
    "  - always use existing well-known entity types. For example, if there is a well-known type \"Person\", and people appear in the text, then propose \"Person\" as the type of entity.\n",
    "  - prefer reusing existing entity types rather than creating new ones\n",
    "  \n",
    "  Design rules for discovered entities:\n",
    "  - discovered entities are consistently mentioned in the text and are highly relevant to the user's goal\n",
    "  - always look for entities that would provide more depth or breadth to the existing graph\n",
    "  - for example, if the user goal is to represent social communities and the graph has \"Person\" nodes, look through the text to discover entities that are relevant like \"Hobby\" or \"Event\"\n",
    "  - avoid quantitative types that may be better represented as a property on an existing entity or relationship.\n",
    "  - for example, do not propose \"Age\" as a type of entity. That is better represented as an additional property \"age\" on a \"Person\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 286
   },
   "outputs": [],
   "source": [
    "ner_agent_chain_of_thought_directions = \"\"\"\n",
    "  Prepare for the task:\n",
    "  - use the 'get_user_goal' tool to get the user goal\n",
    "  - use the 'get_approved_files' tool to get the list of approved files\n",
    "  - use the 'get_well_known_types' tool to get the approved node labels\n",
    "\n",
    "  Think step by step:\n",
    "  1. Sample some of the files using the 'sample_file' tool to understand the content\n",
    "  2. Consider what well-known entities are mentioned in the text\n",
    "  3. Discover entities that are frequently mentioned in the text that support the user's goal\n",
    "  4. Use the 'set_proposed_entities' tool to save the list of well-known and discovered entity types\n",
    "  5. Use the 'get_proposed_entities' tool to retrieve the proposed entities and present them to the user for their approval\n",
    "  6. If the user approves, use the 'approve_proposed_entities' tool to finalize the entity types\n",
    "  7. If the user does not approve, consider their feedback and iterate on the proposal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 131
   },
   "outputs": [],
   "source": [
    "ner_agent_instruction = f\"\"\"\n",
    "{ner_agent_role_and_goal}\n",
    "{ner_agent_hints}\n",
    "{ner_agent_chain_of_thought_directions}\n",
    "\"\"\"\n",
    "\n",
    "#print(ner_agent_instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. Tool Definitions (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous lessons, you'll define some tools that explictly follow\n",
    "a propose then approve pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 490
   },
   "outputs": [],
   "source": [
    "# tools to propose and approve entity types\n",
    "PROPOSED_ENTITIES = \"proposed_entity_types\"\n",
    "APPROVED_ENTITIES = \"approved_entity_types\"\n",
    "\n",
    "def set_proposed_entities(proposed_entity_types: list[str], tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Sets the list proposed entity types to extract from unstructured text.\"\"\"\n",
    "    tool_context.state[PROPOSED_ENTITIES] = proposed_entity_types\n",
    "    return tool_success(PROPOSED_ENTITIES, proposed_entity_types)\n",
    "\n",
    "def get_proposed_entities(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Gets the list of proposed entity types to extract from unstructured text.\"\"\"\n",
    "    return tool_context.state.get(PROPOSED_ENTITIES, [])\n",
    "\n",
    "def approve_proposed_entities(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Upon approval from user, records the proposed entity types as an approved list of entity types \n",
    "\n",
    "    Only call this tool if the user has explicitly approved the suggested files.\n",
    "    \"\"\"\n",
    "    if PROPOSED_ENTITIES not in tool_context.state:\n",
    "        return tool_error(\"No proposed entity types to approve. Please set proposed entities first, ask for user approval, then call this tool.\")\n",
    "    tool_context.state[APPROVED_ENTITIES] = tool_context.state.get(PROPOSED_ENTITIES)\n",
    "    return tool_success(APPROVED_ENTITIES, tool_context.state[APPROVED_ENTITIES])\n",
    "\n",
    "def get_approved_entities(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Get the approved list of entity types to extract from unstructured text.\"\"\"\n",
    "    return tool_context.state.get(APPROVED_ENTITIES, [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"well-known entities\" are based on existing node labels used during graph construction.\n",
    "\n",
    "This helper tool will get the existing node labels from the approved construction plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 167
   },
   "outputs": [],
   "source": [
    "def get_well_known_types(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Gets the approved labels that represent well-known entity types in the graph schema.\"\"\"\n",
    "    construction_plan = tool_context.state.get(\"approved_construction_plan\", {})\n",
    "    # approved labels are the keys for each construction plan entry where `construction_type` is \"node\"\n",
    "    approved_labels = {entry[\"label\"] for entry in construction_plan.values() if entry[\"construction_type\"] == \"node\"}\n",
    "    return tool_success(\"approved_labels\", approved_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full toolset includes some existing tools that you'll import\n",
    "plus the extra tools you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 150
   },
   "outputs": [],
   "source": [
    "from tools import get_approved_user_goal, get_approved_files, sample_file\n",
    "ner_agent_tools = [\n",
    "    get_approved_user_goal, get_approved_files, sample_file,\n",
    "    get_well_known_types,\n",
    "    set_proposed_entities,\n",
    "    approve_proposed_entities\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the NER agent is working with, use the sample_file tool to look\n",
    "at one of the markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 82
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Gothenburg Table Reviews\n",
      "\n",
      "Scraped from https://www.between2furns.com/Gothenburg-Table/dp/B0BQJWJWJW\n",
      "\n",
      "## Rating: â˜…â˜…â˜…â˜…â˜… (5/5)\n",
      "This table is the centerpiece of our dining room! The modern design is stunning yet timeless, and the quality is exceptional. We've had it for 6 months now and it still looks brand new despite daily use with two kids. Assembly took about an hour with two people. The surface is easy to clean and surprisingly resistant to scratches. Highly recommend!\n",
      "\n",
      "- @akollegger (Cambridge)\n",
      "\n",
      "---\n",
      "\n",
      "## Rating: â˜…â˜…â˜…â˜†â˜† (3/5)\n",
      "The Gothenburg Table looks beautiful, I'll give it that. But assembly was a nightmare - some of the pre-drilled holes didn't line up properly and I had to drill new ones. Once assembled, it's sturdy enough, but I expected better quality control for this price point. The table top also shows fingerprints very easily which is annoying for daily use.\n",
      "\n",
      "- @furniture_lover92 (Seattle)\n",
      "\n",
      "---\n",
      "\n",
      "## Rating: â˜…â˜…â˜…â˜…â˜† (4/5)\n",
      "love love LOVE the look of this table!! perfect size for my apartment and the finish is gorgeous. took off a star because it was kinda complicated to put together and the instructions were confusing. but now that it's built it's perfect and all my friends are jealous. definitely worth the struggle lol\n",
      "\n",
      "- @designer_dave (Portland)\n",
      "\n",
      "---\n",
      "\n",
      "## Rating: â˜…â˜…â˜…â˜…â˜… (5/5)\n",
      "As someone who entertains frequently, I needed a table that was both functional and aesthetically pleasing. The Gothenburg Table delivers on both counts. The dimensions (180cm x 90cm) are perfect for seating 6-8 people comfortably. The solid wood construction gives it a substantial feel, and the joinery is impeccable. The finish has a subtle matte quality that highlights the natural grain patterns. Assembly was straightforward with clear instructions. This table will clearly last for decades.\n",
      "\n",
      "- @home_chef (Chicago)\n",
      "\n",
      "---\n",
      "\n",
      "## Rating: â˜…â˜…â˜†â˜†â˜† (2/5)\n",
      "Disappointed with this purchase. The table arrived with a small dent on one edge, and while customer service was helpful in sending a replacement part, it was a hassle to deal with. The table also wobbles slightly on my hardwood floor despite multiple attempts to adjust it. For the price, I expected better quality control.\n",
      "\n",
      "- @diydan (Austin)\n",
      "\n",
      "---\n",
      "\n",
      "## Rating: â˜…â˜…â˜…â˜…â˜… (5/5)\n",
      "Perfect table for our family of four! We've been using it daily for meals, homework, and game nights for the past year. It's held up beautifully with no signs of wear. The surface cleans easily and has resisted stains from art projects and spilled drinks. Assembly was straightforward and the included tools were adequate. The design is simple yet elegant - exactly what we wanted.\n",
      "\n",
      "- @familyfirst (Denver)\n",
      "\n",
      "---\n",
      "\n",
      "## Rating: â˜…â˜…â˜…â˜…â˜† (4/5)\n",
      "Good quality table that looks more expensive than it is. Assembly was straightforward but definitely needs two people. The wood grain is beautiful and the finish is smooth. I'm taking off one star because it scratches a bit more easily than I'd like, but overall I'm happy with the purchase.\n",
      "\n",
      "- @woodworker_amy (Portland)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_result = sample_file(\"product_reviews/gothenburg_table_reviews.md\")\n",
    "\n",
    "print(file_result[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The markdown has product reviews from multiple users that include a rating, the review and their username.\n",
    "\n",
    "For root-cause analysis, you'll be interested in reviews that are negative and report product issues\n",
    "like quality, challenges in assembly, or reliability.\n",
    "\n",
    "We won't provide explicit instructions about product reviews to the agent, instead relying\n",
    "on a combination of the stated user goal along with instruction to find entity types\n",
    "that would support that user goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3. Construct the Sub-agent (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 184
   },
   "outputs": [],
   "source": [
    "NER_AGENT_NAME = \"ner_schema_agent_v1\"\n",
    "ner_schema_agent = Agent(\n",
    "    name=NER_AGENT_NAME,\n",
    "    description=\"Proposes the kind of named entities that could be extracted from text files.\",\n",
    "    model=llm,\n",
    "    instruction=ner_agent_instruction,\n",
    "    tools=ner_agent_tools, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial state is important in this phase, as the agent is designed to act\n",
    "within a particular phase of an overall workflow.\n",
    "\n",
    "The NER agent will need:\n",
    "\n",
    "- the user goal, extended to mention product reviews and what to look for there\n",
    "- a list of markdown files that have been pre-approved\n",
    "- the approved construction plan from the structured data design phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 677
   },
   "outputs": [],
   "source": [
    "ner_agent_initial_state = {\n",
    "    \"approved_user_goal\": {\n",
    "        \"kind_of_graph\": \"supply chain analysis\",\n",
    "        \"description\": \"\"\"A multi-level bill of materials for manufactured products, useful for root cause analysis. \n",
    "        Add product reviews to start analysis from reported issues like quality, difficulty, or durability.\"\"\"\n",
    "    },\n",
    "    \"approved_files\": [\n",
    "        \"product_reviews/gothenburg_table_reviews.md\",\n",
    "        \"product_reviews/helsingborg_dresser_reviews.md\",\n",
    "        \"product_reviews/jonkoping_coffee_table_reviews.md\",\n",
    "        \"product_reviews/linkoping_bed_reviews.md\",\n",
    "        \"product_reviews/malmo_desk_reviews.md\",\n",
    "        \"product_reviews/norrkoping_nightstand_reviews.md\",\n",
    "        \"product_reviews/orebro_lamp_reviews.md\",\n",
    "        \"product_reviews/stockholm_chair_reviews.md\",\n",
    "        \"product_reviews/uppsala_sofa_reviews.md\",\n",
    "        \"product_reviews/vasteras_bookshelf_reviews.md\"\n",
    "    ],\n",
    "    \"approved_construction_plan\": {\n",
    "        \"Product\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Product\",\n",
    "        },\n",
    "        \"Assembly\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Assembly\",\n",
    "        },\n",
    "        \"Part\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Part\",\n",
    "        },\n",
    "        \"Supplier\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Supplier\",\n",
    "        }\n",
    "        # Relationship construction omitted, since it won't get used in this notebook\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, you're ready to run the agent. \n",
    "\n",
    "- use the make_agent_caller to create an execution environment\n",
    "- prompt the agent with a single message that should kick-off the analysis\n",
    "- expect the result to be a proposed list of entity types\n",
    "- but *not* a list of approved entity types\n",
    "\n",
    "**The entity types here may vary quite a bit. If you're not happy with the proposal,\n",
    "you can run the cell again to get a new list.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by LLMs can vary with each execution due to their stochastic nature. Your results might differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 405
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Add product reviews to the knowledge graph to trace product complaints back through the manufacturing process.\n",
      "<<< Agent Response: The proposed entity types have been successfully saved and they align with your goal of integrating product reviews into a supply chain analysis graph. Here's a summary of the proposed entity types:\n",
      "\n",
      "### Well-Known Entities:\n",
      "1. **Product**: Central to tracking and analyzing complaints about specific items.\n",
      "2. **Supplier**: Relevant for linking defects or issues back to the origin.\n",
      "3. **Assembly**: Important for understanding issues encountered during product assembly.\n",
      "\n",
      "### Discovered Entities:\n",
      "1. **CustomerReview**: Represents individual reviews, capturing sentiments and specific issues.\n",
      "2. **Issue**: Captures particular concerns like \"Ease of Assembly\" or \"Quality Control.\"\n",
      "3. **CustomerExperience**: Tracks overall sentiment and recurring themes in customer feedback.\n",
      "4. **Location**: Enables regional analysis of reported problems.\n",
      "\n",
      "Would you like to proceed with these entity types, or is there anything else you'd like to adjust?\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Session state:  {'approved_user_goal': {'kind_of_graph': 'supply chain analysis', 'description': 'A multi-level bill of materials for manufactured products, useful for root cause analysis. \\n        Add product reviews to start analysis from reported issues like quality, difficulty, or durability.'}, 'approved_files': ['product_reviews/gothenburg_table_reviews.md', 'product_reviews/helsingborg_dresser_reviews.md', 'product_reviews/jonkoping_coffee_table_reviews.md', 'product_reviews/linkoping_bed_reviews.md', 'product_reviews/malmo_desk_reviews.md', 'product_reviews/norrkoping_nightstand_reviews.md', 'product_reviews/orebro_lamp_reviews.md', 'product_reviews/stockholm_chair_reviews.md', 'product_reviews/uppsala_sofa_reviews.md', 'product_reviews/vasteras_bookshelf_reviews.md'], 'approved_construction_plan': {'Product': {'construction_type': 'node', 'label': 'Product'}, 'Assembly': {'construction_type': 'node', 'label': 'Assembly'}, 'Part': {'construction_type': 'node', 'label': 'Part'}, 'Supplier': {'construction_type': 'node', 'label': 'Supplier'}}, 'proposed_entity_types': ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location']}\n",
      "\n",
      "Proposed entities:  ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location']\n",
      "\n",
      "Awaiting approval.\n"
     ]
    }
   ],
   "source": [
    "from helper import make_agent_caller\n",
    "\n",
    "ner_agent_caller = await make_agent_caller(ner_schema_agent, ner_agent_initial_state)\n",
    "\n",
    "await ner_agent_caller.call(\"Add product reviews to the knowledge graph to trace product complaints back through the manufacturing process.\")\n",
    "\n",
    "# Alternatively, uncomment this line to get verbose output\n",
    "# await ner_agent_caller.call(\"Add product reviews.\", True)\n",
    "\n",
    "session_end = await ner_agent_caller.get_session()\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"\\nSession state: \", session_end.state)\n",
    "\n",
    "if PROPOSED_ENTITIES in session_end.state:\n",
    "    print(\"\\nProposed entities: \", session_end.state[PROPOSED_ENTITIES])\n",
    "\n",
    "if APPROVED_ENTITIES in session_end.state:\n",
    "    print(\"\\nInappropriately approved entities: \", session_end.state[APPROVED_ENTITIES])\n",
    "else:\n",
    "    print(\"\\nAwaiting approval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "- often, the agent will confuse the process of \"Assembly\" with the resulting thing that is an \"Assembly\"\n",
    "- why is that?\n",
    "- the agent will see the term \"Assembly\" in the list of well-known entity types\n",
    "- and, it will notice complaints about assembling furniture\n",
    "- but, it has no way to know those are two different uses of the word\n",
    "- to fix this, the schema proposal from the previous lesson could save descriptions for each node label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're happy with the proposal, you can tell the agent that you approve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 235
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Approve the proposed entities.\n",
      "<<< Agent Response: The proposed entities have been successfully approved and will be used to enhance the supply chain analysis knowledge graph. These entity types will help trace product complaints back through the manufacturing process more effectively. If there's anything else you need, feel free to let me know!\n",
      "Session state:  {'approved_user_goal': {'kind_of_graph': 'supply chain analysis', 'description': 'A multi-level bill of materials for manufactured products, useful for root cause analysis. \\n        Add product reviews to start analysis from reported issues like quality, difficulty, or durability.'}, 'approved_files': ['product_reviews/gothenburg_table_reviews.md', 'product_reviews/helsingborg_dresser_reviews.md', 'product_reviews/jonkoping_coffee_table_reviews.md', 'product_reviews/linkoping_bed_reviews.md', 'product_reviews/malmo_desk_reviews.md', 'product_reviews/norrkoping_nightstand_reviews.md', 'product_reviews/orebro_lamp_reviews.md', 'product_reviews/stockholm_chair_reviews.md', 'product_reviews/uppsala_sofa_reviews.md', 'product_reviews/vasteras_bookshelf_reviews.md'], 'approved_construction_plan': {'Product': {'construction_type': 'node', 'label': 'Product'}, 'Assembly': {'construction_type': 'node', 'label': 'Assembly'}, 'Part': {'construction_type': 'node', 'label': 'Part'}, 'Supplier': {'construction_type': 'node', 'label': 'Supplier'}}, 'proposed_entity_types': ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location'], 'approved_entity_types': ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location']}\n",
      "\n",
      "Approved entities:  ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location']\n"
     ]
    }
   ],
   "source": [
    "await ner_agent_caller.call(\"Approve the proposed entities.\")\n",
    "\n",
    "session_end = await ner_agent_caller.get_session()\n",
    "\n",
    "ner_end_state = session_end.state if session_end else {}\n",
    "\n",
    "print(\"Session state: \", ner_end_state)\n",
    "\n",
    "if APPROVED_ENTITIES in ner_end_state:\n",
    "    print(\"\\nApproved entities: \", ner_end_state[APPROVED_ENTITIES])\n",
    "else:\n",
    "    print(\"\\nStill awaiting approval? That is weird. Please check the agent's state and the proposed entities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Fact Type Extraction Sub-agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. Agent Instructions (fact type extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 116
   },
   "outputs": [],
   "source": [
    "fact_agent_role_and_goal = \"\"\"\n",
    "  You are a top-tier algorithm designed for analyzing text files and proposing\n",
    "  the type of facts that could be extracted from text that would be relevant \n",
    "  for a user's goal. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 303
   },
   "outputs": [],
   "source": [
    "fact_agent_hints = \"\"\"\n",
    "  Do not propose specific individual facts, but instead propose the general type \n",
    "  of facts that would be relevant for the user's goal. \n",
    "  For example, do not propose \"ABK likes coffee\" but the general type of fact \"Person likes Beverage\".\n",
    "  \n",
    "  Facts are triplets of (subject, predicate, object) where the subject and object are\n",
    "  approved entity types, and the proposed predicate provides information about\n",
    "  how they are related. For example, a fact type could be (Person, likes, Beverage).\n",
    "\n",
    "  Design rules for facts:\n",
    "  - only use approved entity types as subjects or objects. Do not propose new types of entities\n",
    "  - the proposed predicate should describe the relationship between the approved subject and object\n",
    "  - the predicate should optimize for information that is relevant to the user's goal\n",
    "  - the predicate must appear in the source text. Do not guess.\n",
    "  - use the 'add_proposed_fact' tool to record each proposed fact type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 269
   },
   "outputs": [],
   "source": [
    "fact_agent_chain_of_thought_directions = \"\"\"\n",
    "    Prepare for the task:\n",
    "    - use the 'get_approved_user_goal' tool to get the user goal\n",
    "    - use the 'get_approved_files' tool to get the list of approved files\n",
    "    - use the 'get_approved_entities' tool to get the list of approved entity types\n",
    "\n",
    "    Think step by step:\n",
    "    1. Use the 'get_approved_user_goal' tool to get the user goal\n",
    "    2. Sample some of the approved files using the 'sample_file' tool to understand the content\n",
    "    3. Consider how subjects and objects are related in the text\n",
    "    4. Call the 'add_proposed_fact' tool for each type of fact you propose\n",
    "    5. Use the 'get_proposed_facts' tool to retrieve all the proposed facts\n",
    "    6. Present the proposed types of facts to the user, along with an explanation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 97
   },
   "outputs": [],
   "source": [
    "fact_agent_instruction = f\"\"\"\n",
    "{fact_agent_role_and_goal}\n",
    "{fact_agent_hints}\n",
    "{fact_agent_chain_of_thought_directions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. Tool Definitions (fact type extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 881
   },
   "outputs": [],
   "source": [
    "PROPOSED_FACTS = \"proposed_fact_types\"\n",
    "APPROVED_FACTS = \"approved_fact_types\"\n",
    "\n",
    "def add_proposed_fact(approved_subject_label:str,\n",
    "                      proposed_predicate_label:str,\n",
    "                      approved_object_label:str,\n",
    "                      tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Add a proposed type of fact that could be extracted from the files.\n",
    "\n",
    "    A proposed fact type is a tuple of (subject, predicate, object) where\n",
    "    the subject and object are approved entity types and the predicate \n",
    "    is a proposed relationship label.\n",
    "\n",
    "    Args:\n",
    "      approved_subject_label: approved label of the subject entity type\n",
    "      proposed_predicate_label: label of the predicate\n",
    "      approved_object_label: approved label of the object entity type\n",
    "    \"\"\"\n",
    "    # Guard against invalid labels\n",
    "    approved_entities = tool_context.state.get(APPROVED_ENTITIES, [])\n",
    "    \n",
    "    if approved_subject_label not in approved_entities:\n",
    "        return tool_error(f\"Approved subject label {approved_subject_label} not found. Try again.\")\n",
    "    if approved_object_label not in approved_entities:\n",
    "        return tool_error(f\"Approved object label {approved_object_label} not found. Try again.\")\n",
    "    \n",
    "    current_predicates = tool_context.state.get(PROPOSED_FACTS, {})\n",
    "    current_predicates[proposed_predicate_label] = {\n",
    "        \"subject_label\": approved_subject_label,\n",
    "        \"predicate_label\": proposed_predicate_label,\n",
    "        \"object_label\": approved_object_label\n",
    "    }\n",
    "    tool_context.state[PROPOSED_FACTS] = current_predicates\n",
    "    return tool_success(PROPOSED_FACTS, current_predicates)\n",
    "    \n",
    "def get_proposed_facts(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Get the proposed types of facts that could be extracted from the files.\"\"\"\n",
    "    return tool_context.state.get(PROPOSED_FACTS, {})\n",
    "\n",
    "\n",
    "def approve_proposed_facts(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Upon user approval, records the proposed fact types as approved fact types\n",
    "\n",
    "    Only call this tool if the user has explicitly approved the proposed fact types.\n",
    "    \"\"\"\n",
    "    if PROPOSED_FACTS not in tool_context.state:\n",
    "        return tool_error(\"No proposed fact types to approve. Please set proposed facts first, ask for user approval, then call this tool.\")\n",
    "    tool_context.state[APPROVED_FACTS] = tool_context.state.get(PROPOSED_FACTS)\n",
    "    return tool_success(APPROVED_FACTS, tool_context.state[APPROVED_FACTS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "height": 148
   },
   "outputs": [],
   "source": [
    "fact_agent_tools = [\n",
    "    get_approved_user_goal, get_approved_files, \n",
    "    get_approved_entities,\n",
    "    sample_file,\n",
    "    add_proposed_fact,\n",
    "    get_proposed_facts,\n",
    "    approve_proposed_facts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3. Construct the Sub-agent (fact type extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 184
   },
   "outputs": [],
   "source": [
    "FACT_AGENT_NAME = \"fact_type_extraction_agent_v1\"\n",
    "relevant_fact_agent = Agent(\n",
    "    name=FACT_AGENT_NAME,\n",
    "    description=\"Proposes the kind of relevant facts that could be extracted from text files.\",\n",
    "    model=llm,\n",
    "    instruction=fact_agent_instruction,\n",
    "    tools=fact_agent_tools, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by LLMs can vary with each execution due to their stochastic nature. Your results might differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "height": 490
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Propose fact types that can be found in the text.\n",
      "<<< Agent Response: I have successfully proposed several types of facts that can be extracted from the text files. Here they are, along with their relevance to the user's goal of supply chain analysis:\n",
      "\n",
      "1. **(CustomerReview, mentionsIssue, Issue)**\n",
      "   - Extracts issues mentioned in customer reviews to help identify common product problems and quality control issues.\n",
      "\n",
      "2. **(Product, requiresAssembly, Assembly)**\n",
      "   - Captures information about whether a product requires assembly and any related challenges, offering insights into user experience and potential assembly-related complaints. \n",
      "\n",
      "3. **(CustomerReview, relatesToProduct, Product)**\n",
      "   - Associates customer reviews with the products being reviewed, allowing for an understanding of customer satisfaction and specific feedback on product performance.\n",
      "\n",
      "4. **(Issue, impactsProductQuality, Product)**\n",
      "   - Focuses on how identified issues impact the perceived quality of the product, facilitating root cause analysis for quality management.\n",
      "\n",
      "5. **(Product, producedBySupplier, Supplier)**\n",
      "   - Although not found directly in the sample text, this fact type would help assign responsibility for product issues within the supply chain analysis.\n",
      "\n",
      "6. **(CustomerReview, locatedAt, Location)**\n",
      "   - Extracts the location of the reviewer, aiding in geographic analysis of product satisfaction which might reveal regional trends in quality perceptions or issues.\n",
      "\n",
      "Unfortunately, the fact type **(Product, hasRating, Rating)** could not be proposed due to the absence of an approved \"Rating\" entity type. If you wish to add that or need any modifications, please let me know!\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Session state:  {'approved_user_goal': {'kind_of_graph': 'supply chain analysis', 'description': 'A multi-level bill of materials for manufactured products, useful for root cause analysis. \\n        Add product reviews to start analysis from reported issues like quality, difficulty, or durability.'}, 'approved_files': ['product_reviews/gothenburg_table_reviews.md', 'product_reviews/helsingborg_dresser_reviews.md', 'product_reviews/jonkoping_coffee_table_reviews.md', 'product_reviews/linkoping_bed_reviews.md', 'product_reviews/malmo_desk_reviews.md', 'product_reviews/norrkoping_nightstand_reviews.md', 'product_reviews/orebro_lamp_reviews.md', 'product_reviews/stockholm_chair_reviews.md', 'product_reviews/uppsala_sofa_reviews.md', 'product_reviews/vasteras_bookshelf_reviews.md'], 'approved_construction_plan': {'Product': {'construction_type': 'node', 'label': 'Product'}, 'Assembly': {'construction_type': 'node', 'label': 'Assembly'}, 'Part': {'construction_type': 'node', 'label': 'Part'}, 'Supplier': {'construction_type': 'node', 'label': 'Supplier'}}, 'proposed_entity_types': ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location'], 'approved_entity_types': ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location'], 'proposed_fact_types': {'mentionsIssue': {'subject_label': 'CustomerReview', 'predicate_label': 'mentionsIssue', 'object_label': 'Issue'}, 'requiresAssembly': {'subject_label': 'Product', 'predicate_label': 'requiresAssembly', 'object_label': 'Assembly'}, 'relatesToProduct': {'subject_label': 'CustomerReview', 'predicate_label': 'relatesToProduct', 'object_label': 'Product'}, 'impactsProductQuality': {'subject_label': 'Issue', 'predicate_label': 'impactsProductQuality', 'object_label': 'Product'}, 'producedBySupplier': {'subject_label': 'Product', 'predicate_label': 'producedBySupplier', 'object_label': 'Supplier'}, 'locatedAt': {'subject_label': 'CustomerReview', 'predicate_label': 'locatedAt', 'object_label': 'Location'}}}\n",
      "\n",
      "Approved entities:  ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location']\n",
      "\n",
      "Correctly proposed facts:  {'mentionsIssue': {'subject_label': 'CustomerReview', 'predicate_label': 'mentionsIssue', 'object_label': 'Issue'}, 'requiresAssembly': {'subject_label': 'Product', 'predicate_label': 'requiresAssembly', 'object_label': 'Assembly'}, 'relatesToProduct': {'subject_label': 'CustomerReview', 'predicate_label': 'relatesToProduct', 'object_label': 'Product'}, 'impactsProductQuality': {'subject_label': 'Issue', 'predicate_label': 'impactsProductQuality', 'object_label': 'Product'}, 'producedBySupplier': {'subject_label': 'Product', 'predicate_label': 'producedBySupplier', 'object_label': 'Supplier'}, 'locatedAt': {'subject_label': 'CustomerReview', 'predicate_label': 'locatedAt', 'object_label': 'Location'}}\n",
      "\n",
      "Approved facts not found in session state, which is good.\n"
     ]
    }
   ],
   "source": [
    "# make a copy of the NER agent's end state to use as the initial state for the fact agent\n",
    "fact_agent_initial_state = ner_end_state.copy()\n",
    "\n",
    "fact_agent_caller = await make_agent_caller(relevant_fact_agent, fact_agent_initial_state)\n",
    "\n",
    "await fact_agent_caller.call(\"Propose fact types that can be found in the text.\")\n",
    "# await fact_agent_caller.call(\"Propose fact types that can be found in the text.\", True)\n",
    "\n",
    "session_end = await fact_agent_caller.get_session()\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"\\nSession state: \", session_end.state)\n",
    "\n",
    "print(\"\\nApproved entities: \", session_end.state.get(APPROVED_ENTITIES, []))\n",
    "\n",
    "# Check that the agent proposed facts\n",
    "if PROPOSED_FACTS in session_end.state:\n",
    "    print(\"\\nCorrectly proposed facts: \", session_end.state[PROPOSED_FACTS])\n",
    "else:\n",
    "    print(\"\\nProposed facts not found in session state. What went wrong?\")\n",
    "\n",
    "# Check that the agent did not inappropriately approve facts\n",
    "if APPROVED_FACTS in session_end.state:\n",
    "    print(\"\\nInappriately approved facts: \", session_end.state[APPROVED_FACTS])\n",
    "else:\n",
    "    print(\"\\nApproved facts not found in session state, which is good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're happy with the fact type proposal, approve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "height": 201
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Approve the proposed fact types.\n",
      "<<< Agent Response: The proposed fact types have already been approved. If there is anything else you need assistance with, please let me know!\n",
      "Session state:  {'approved_user_goal': {'kind_of_graph': 'supply chain analysis', 'description': 'A multi-level bill of materials for manufactured products, useful for root cause analysis. \\n        Add product reviews to start analysis from reported issues like quality, difficulty, or durability.'}, 'approved_files': ['product_reviews/gothenburg_table_reviews.md', 'product_reviews/helsingborg_dresser_reviews.md', 'product_reviews/jonkoping_coffee_table_reviews.md', 'product_reviews/linkoping_bed_reviews.md', 'product_reviews/malmo_desk_reviews.md', 'product_reviews/norrkoping_nightstand_reviews.md', 'product_reviews/orebro_lamp_reviews.md', 'product_reviews/stockholm_chair_reviews.md', 'product_reviews/uppsala_sofa_reviews.md', 'product_reviews/vasteras_bookshelf_reviews.md'], 'approved_construction_plan': {'Product': {'construction_type': 'node', 'label': 'Product'}, 'Assembly': {'construction_type': 'node', 'label': 'Assembly'}, 'Part': {'construction_type': 'node', 'label': 'Part'}, 'Supplier': {'construction_type': 'node', 'label': 'Supplier'}}, 'proposed_entity_types': ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location'], 'approved_entity_types': ['Product', 'Supplier', 'Assembly', 'CustomerReview', 'Issue', 'CustomerExperience', 'Location'], 'proposed_fact_types': {'mentionsIssue': {'subject_label': 'CustomerReview', 'predicate_label': 'mentionsIssue', 'object_label': 'Issue'}, 'requiresAssembly': {'subject_label': 'Product', 'predicate_label': 'requiresAssembly', 'object_label': 'Assembly'}, 'relatesToProduct': {'subject_label': 'CustomerReview', 'predicate_label': 'relatesToProduct', 'object_label': 'Product'}, 'impactsProductQuality': {'subject_label': 'Issue', 'predicate_label': 'impactsProductQuality', 'object_label': 'Product'}, 'producedBySupplier': {'subject_label': 'Product', 'predicate_label': 'producedBySupplier', 'object_label': 'Supplier'}, 'locatedAt': {'subject_label': 'CustomerReview', 'predicate_label': 'locatedAt', 'object_label': 'Location'}}, 'approved_fact_types': {'mentionsIssue': {'subject_label': 'CustomerReview', 'predicate_label': 'mentionsIssue', 'object_label': 'Issue'}, 'requiresAssembly': {'subject_label': 'Product', 'predicate_label': 'requiresAssembly', 'object_label': 'Assembly'}, 'relatesToProduct': {'subject_label': 'CustomerReview', 'predicate_label': 'relatesToProduct', 'object_label': 'Product'}, 'impactsProductQuality': {'subject_label': 'Issue', 'predicate_label': 'impactsProductQuality', 'object_label': 'Product'}, 'producedBySupplier': {'subject_label': 'Product', 'predicate_label': 'producedBySupplier', 'object_label': 'Supplier'}, 'locatedAt': {'subject_label': 'CustomerReview', 'predicate_label': 'locatedAt', 'object_label': 'Location'}}}\n",
      "\n",
      "Approved fact types:  {'mentionsIssue': {'subject_label': 'CustomerReview', 'predicate_label': 'mentionsIssue', 'object_label': 'Issue'}, 'requiresAssembly': {'subject_label': 'Product', 'predicate_label': 'requiresAssembly', 'object_label': 'Assembly'}, 'relatesToProduct': {'subject_label': 'CustomerReview', 'predicate_label': 'relatesToProduct', 'object_label': 'Product'}, 'impactsProductQuality': {'subject_label': 'Issue', 'predicate_label': 'impactsProductQuality', 'object_label': 'Product'}, 'producedBySupplier': {'subject_label': 'Product', 'predicate_label': 'producedBySupplier', 'object_label': 'Supplier'}, 'locatedAt': {'subject_label': 'CustomerReview', 'predicate_label': 'locatedAt', 'object_label': 'Location'}}\n"
     ]
    }
   ],
   "source": [
    "await fact_agent_caller.call(\"Approve the proposed fact types.\")\n",
    "\n",
    "session_end = await fact_agent_caller.get_session()\n",
    "\n",
    "print(\"Session state: \", session_end.state)\n",
    "\n",
    "if APPROVED_FACTS in session_end.state:\n",
    "    print(\"\\nApproved fact types: \", session_end.state[APPROVED_FACTS])\n",
    "else:\n",
    "    print(\"\\nFailed to approve fact types.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
