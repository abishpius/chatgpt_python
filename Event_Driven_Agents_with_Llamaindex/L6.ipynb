{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cde6753-c327-4c1d-9ea8-63a1f851ddff",
   "metadata": {},
   "source": [
    "# Lesson 6: Use your voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121aec5-1316-4186-aad5-914aecb6c6cf",
   "metadata": {},
   "source": [
    "**Lesson objective**: Get voice feedback \n",
    "\n",
    "So far we've set up a moderately complex workflow with a human feedback loop. Let's run it through the visualizer to see what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456a5ef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff1d7; padding:15px;\"> <b> Note</b>: Make sure to run the notebook cell by cell. Please try to avoid running all cells at once.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497254a3-476d-401f-9d40-76e9a364756c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180fcb51-4d47-4877-a5b3-f773c64f1f72",
   "metadata": {
    "height": 572
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent\n",
    ")\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.readers.whisper import WhisperReader\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from queue import Queue\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from helper import get_openai_api_key, get_llama_cloud_api_key\n",
    "\n",
    "llama_cloud_api_key = get_llama_cloud_api_key()\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3a2f78-d56f-4bbb-9664-3da5f90cf102",
   "metadata": {
    "height": 2918
   },
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    response: str\n",
    "\n",
    "class FeedbackEvent(Event):\n",
    "    feedback: str\n",
    "\n",
    "class GenerateQuestionsEvent(Event):\n",
    "    pass\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    storage_dir = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: VectorStoreIndex\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # give ourselves an LLM to work with\n",
    "        self.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "        # ingest our data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # we've already ingested our documents\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            # we need to parse and load our documents\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=\"markdown\",\n",
    "                content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            ).load_data(ev.resume_file)\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # either way, create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # let's pass our application form to a new step where we parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # we've separated the form parsing from the question generation\n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        await ctx.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # this step can get triggered either by GenerateQuestionsEvent or a FeedbackEvent\n",
    "    @step\n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            if hasattr(ev,\"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "\n",
    "            ctx.send_event(QueryEvent(\n",
    "                field=field,\n",
    "                query=question\n",
    "            ))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.set(\"total_fields\", len(fields))\n",
    "        return\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        print(f\"Asking question: {ev.query}\")\n",
    "\n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
    "\n",
    "        print(f\"Answer was: {str(response)}\")\n",
    "\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "    # we now emit an InputRequiredEvent\n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\")\n",
    "\n",
    "        # save the result for later\n",
    "        await ctx.set(\"filled_form\", str(result))\n",
    "\n",
    "        # Let's get a human in the loop\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result\n",
    "        )\n",
    "\n",
    "    # Accept the feedback.\n",
    "    @step\n",
    "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if (verdict == \"OKAY\"):\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0039610-c02e-4e7e-9c9f-1dc56450543d",
   "metadata": {
    "height": 62
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.ResponseEvent'>\n",
      "<class 'llama_index.core.workflow.events.InputRequiredEvent'>\n",
      "<class '__main__.QueryEvent'>\n",
      "<class '__main__.FeedbackEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.GenerateQuestionsEvent'>\n",
      "<class '__main__.ParseFormEvent'>\n",
      "workflows/lesson_6.html\n"
     ]
    }
   ],
   "source": [
    "WORKFLOW_FILE = \"workflows/lesson_6.html\"\n",
    "draw_all_possible_flows(RAGWorkflow, filename=WORKFLOW_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777454c5-31d0-4b23-ac03-f74207fbaf83",
   "metadata": {
    "height": 113
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <div style=\"width: 100%; height: 800px; overflow: hidden;\"> <html>\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        \n",
       "            <script src=\"lib/bindings/utils.js\"></script>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
       "            \n",
       "        \n",
       "<center>\n",
       "<h1></h1>\n",
       "</center>\n",
       "\n",
       "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
       "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
       "        <link\n",
       "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
       "          rel=\"stylesheet\"\n",
       "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        />\n",
       "        <script\n",
       "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
       "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        ></script>\n",
       "\n",
       "\n",
       "        <center>\n",
       "          <h1></h1>\n",
       "        </center>\n",
       "        <style type=\"text/css\">\n",
       "\n",
       "             #mynetwork {\n",
       "                 width: 100%;\n",
       "                 height: 750px;\n",
       "                 background-color: #ffffff;\n",
       "                 border: 1px solid lightgray;\n",
       "                 position: relative;\n",
       "                 float: left;\n",
       "             }\n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "        </style>\n",
       "    </head>\n",
       "\n",
       "\n",
       "    <body>\n",
       "        <div class=\"card\" style=\"width: 100%\">\n",
       "            \n",
       "            \n",
       "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        <script type=\"text/javascript\">\n",
       "\n",
       "              // initialize global variables.\n",
       "              var edges;\n",
       "              var nodes;\n",
       "              var allNodes;\n",
       "              var allEdges;\n",
       "              var nodeColors;\n",
       "              var originalNodes;\n",
       "              var network;\n",
       "              var container;\n",
       "              var options, data;\n",
       "              var filter = {\n",
       "                  item : '',\n",
       "                  property : '',\n",
       "                  value : []\n",
       "              };\n",
       "\n",
       "              \n",
       "\n",
       "              \n",
       "\n",
       "              // This method is responsible for drawing the graph, returns the drawn network\n",
       "              function drawGraph() {\n",
       "                  var container = document.getElementById('mynetwork');\n",
       "\n",
       "                  \n",
       "\n",
       "                  // parsing and collecting nodes and edges from the python\n",
       "                  nodes = new vis.DataSet([{\"color\": \"#FFA07A\", \"id\": \"StopEvent\", \"label\": \"StopEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"_done\", \"label\": \"_done\", \"shape\": \"box\"}, {\"color\": \"#ADD8E6\", \"id\": \"ask_question\", \"label\": \"ask_question\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"QueryEvent\", \"label\": \"QueryEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"fill_in_application\", \"label\": \"fill_in_application\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"ResponseEvent\", \"label\": \"ResponseEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#90EE90\", \"id\": \"InputRequiredEvent\", \"label\": \"InputRequiredEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#BEDAE4\", \"id\": \"external_step\", \"label\": \"external_step\", \"shape\": \"box\"}, {\"color\": \"#ADD8E6\", \"id\": \"generate_questions\", \"label\": \"generate_questions\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"GenerateQuestionsEvent\", \"label\": \"GenerateQuestionsEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#90EE90\", \"id\": \"FeedbackEvent\", \"label\": \"FeedbackEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"get_feedback\", \"label\": \"get_feedback\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"HumanResponseEvent\", \"label\": \"HumanResponseEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"parse_form\", \"label\": \"parse_form\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"ParseFormEvent\", \"label\": \"ParseFormEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"set_up\", \"label\": \"set_up\", \"shape\": \"box\"}, {\"color\": \"#E27AFF\", \"id\": \"StartEvent\", \"label\": \"StartEvent\", \"shape\": \"ellipse\"}]);\n",
       "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"StopEvent\", \"to\": \"_done\"}, {\"arrows\": \"to\", \"from\": \"ask_question\", \"to\": \"ResponseEvent\"}, {\"arrows\": \"to\", \"from\": \"QueryEvent\", \"to\": \"ask_question\"}, {\"arrows\": \"to\", \"from\": \"fill_in_application\", \"to\": \"InputRequiredEvent\"}, {\"arrows\": \"to\", \"from\": \"InputRequiredEvent\", \"to\": \"external_step\"}, {\"arrows\": \"to\", \"from\": \"ResponseEvent\", \"to\": \"fill_in_application\"}, {\"arrows\": \"to\", \"from\": \"generate_questions\", \"to\": \"QueryEvent\"}, {\"arrows\": \"to\", \"from\": \"GenerateQuestionsEvent\", \"to\": \"generate_questions\"}, {\"arrows\": \"to\", \"from\": \"FeedbackEvent\", \"to\": \"generate_questions\"}, {\"arrows\": \"to\", \"from\": \"get_feedback\", \"to\": \"FeedbackEvent\"}, {\"arrows\": \"to\", \"from\": \"get_feedback\", \"to\": \"StopEvent\"}, {\"arrows\": \"to\", \"from\": \"HumanResponseEvent\", \"to\": \"get_feedback\"}, {\"arrows\": \"to\", \"from\": \"external_step\", \"to\": \"HumanResponseEvent\"}, {\"arrows\": \"to\", \"from\": \"parse_form\", \"to\": \"GenerateQuestionsEvent\"}, {\"arrows\": \"to\", \"from\": \"ParseFormEvent\", \"to\": \"parse_form\"}, {\"arrows\": \"to\", \"from\": \"set_up\", \"to\": \"ParseFormEvent\"}, {\"arrows\": \"to\", \"from\": \"StartEvent\", \"to\": \"set_up\"}]);\n",
       "\n",
       "                  nodeColors = {};\n",
       "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "                  for (nodeId in allNodes) {\n",
       "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "                  }\n",
       "                  allEdges = edges.get({ returnType: \"Object\" });\n",
       "                  // adding nodes and edges to the graph\n",
       "                  data = {nodes: nodes, edges: edges};\n",
       "\n",
       "                  var options = {\n",
       "    \"configure\": {\n",
       "        \"enabled\": false\n",
       "    },\n",
       "    \"edges\": {\n",
       "        \"color\": {\n",
       "            \"inherit\": true\n",
       "        },\n",
       "        \"smooth\": {\n",
       "            \"enabled\": true,\n",
       "            \"type\": \"dynamic\"\n",
       "        }\n",
       "    },\n",
       "    \"interaction\": {\n",
       "        \"dragNodes\": true,\n",
       "        \"hideEdgesOnDrag\": false,\n",
       "        \"hideNodesOnDrag\": false\n",
       "    },\n",
       "    \"physics\": {\n",
       "        \"enabled\": true,\n",
       "        \"stabilization\": {\n",
       "            \"enabled\": true,\n",
       "            \"fit\": true,\n",
       "            \"iterations\": 1000,\n",
       "            \"onlyDynamicEdges\": false,\n",
       "            \"updateInterval\": 50\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  network = new vis.Network(container, data, options);\n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  return network;\n",
       "\n",
       "              }\n",
       "              drawGraph();\n",
       "        </script>\n",
       "    </body>\n",
       "</html> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "isolated": true
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, DisplayHandle\n",
    "from helper import extract_html_content\n",
    "\n",
    "html_content = extract_html_content(WORKFLOW_FILE)\n",
    "display(HTML(html_content), metadata=dict(isolated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d55ed9-88d0-44ac-b4a3-eb605bcb0638",
   "metadata": {},
   "source": [
    "Cool! You can see the path all the way to the end and the feedback loop is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1147987",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>To access <code>fake_application_form.pdf</code>, <code>fake_resume.pdf</code>, <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. The form and resume are inside the data folder.\n",
    "\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips and Help\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a258f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c82425-bdda-4f93-9226-fc4e4834d8ef",
   "metadata": {},
   "source": [
    "## Getting voice feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fad645-47db-436b-84e2-914acc89dee3",
   "metadata": {},
   "source": [
    "Now, just for fun, you'll do one more thing: change the feedback from text feedback to actual words spoken out loud. To do this we'll use a different model from OpenAI called Whisper. LlamaIndex has a built-in way to transcribe audio files into text using Whisper.\n",
    "\n",
    "Here's a function that takes a file and uses Whisper to return just the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53149fea-7383-4c39-a58c-5836857f69b7",
   "metadata": {
    "height": 198
   },
   "outputs": [],
   "source": [
    "def transcribe_speech(filepath):\n",
    "    if filepath is None:\n",
    "        gr.Warning(\"No audio found, please retry.\")\n",
    "    audio_file= open(filepath, \"rb\")\n",
    "    reader = WhisperReader(\n",
    "        model=\"whisper-1\",\n",
    "        api_key=openai_api_key,\n",
    "    )\n",
    "    documents = reader.load_data(filepath)\n",
    "    return documents[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b67c70a-48a6-4fcd-bf18-2c390cce5855",
   "metadata": {},
   "source": [
    "But before we can use it, you need to capture some audio from your microphone. That involves some extra steps!\n",
    "\n",
    "First, create a callback function that saves data to a global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53efc82-cbd2-448d-bc6f-96d6bc1486f7",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "def store_transcription(output):\n",
    "    global transcription_value\n",
    "    transcription_value = output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e3292-2d56-450c-bcb1-65b510a994ee",
   "metadata": {},
   "source": [
    "Now use Gradio, which has special widgets that can render inside a notebook, to create an interface for capturing audio from a microphone. When the audio is captured, it calls `transcribe_speech` on the recorded data, and calls `store_transcription` on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e38ac6-1e88-46dd-ace4-77241aff9d42",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "mic_transcribe = gr.Interface(\n",
    "    fn=lambda x: store_transcription(transcribe_speech(x)),\n",
    "    inputs=gr.Audio(sources=\"microphone\",\n",
    "                    type=\"filepath\"),\n",
    "    outputs=gr.Textbox(label=\"Transcription\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e04512-d9ab-450d-850f-072ae67bc451",
   "metadata": {},
   "source": [
    "In Gradio, you further define a visual interface containing this microphone input and output, and then launch it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189076f4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff1d7; padding:15px;\"> <b> Note</b>: Make sure to wait for the gradio interface to load. A popup window will appear and ask you to allow the use of your microphone. To record audio, make sure to click on record -> stop -> submit. Make sure the audio is captured before clicking on 'submit'.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc89e72-76e0-4cfd-904b-0d3dd058532b",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  https://0.0.0.0:8000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://s172-29-96-78p8000.lab-aws-production.deeplearning.ai/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interface = gr.Blocks()\n",
    "with test_interface:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe],\n",
    "        [\"Transcribe Microphone\"]\n",
    "    )\n",
    "\n",
    "test_interface.launch(\n",
    "    share=False, \n",
    "    server_port=8000, \n",
    "    prevent_thread_lock=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30d95b-47bb-45db-9d42-5835120d92ae",
   "metadata": {},
   "source": [
    "You can now print out the transcription, which is stored in that global variable you created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5de79c4-dbda-4103-baa1-5b315af6c1e1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transcription_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtranscription_value\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transcription_value' is not defined"
     ]
    }
   ],
   "source": [
    "print(transcription_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ee4ba-f744-4b7c-906f-b26151700d27",
   "metadata": {},
   "source": [
    "You're going to want to run Gradio again, so it's a good idea to shut down the Gradio interface you were using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436f51ba-760d-4af6-8da1-8088e7c8ebcd",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 8000\n"
     ]
    }
   ],
   "source": [
    "test_interface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0f309",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff1d7; padding:15px;\"> <b> Note</b>: Make sure to run the previous cell to close the Gradio interface before running the next cell.</div>                                                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b05ec0-495f-4376-a167-43efc1db04cd",
   "metadata": {},
   "source": [
    "Now you're going to create an entirely new class, a Transcription Handler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f30a2e-2c39-411a-917c-b78549e5eef1",
   "metadata": {
    "height": 827
   },
   "outputs": [],
   "source": [
    "# New! Transcription handler.\n",
    "class TranscriptionHandler:\n",
    "\n",
    "    # we create a queue to hold transcription values\n",
    "    def __init__(self):\n",
    "        self.transcription_queue = Queue()\n",
    "        self.interface = None\n",
    "\n",
    "    # every time we record something we put it in the queue\n",
    "    def store_transcription(self, output):\n",
    "        self.transcription_queue.put(output)\n",
    "        return output\n",
    "\n",
    "    # This is the same interface and transcription logic as before\n",
    "    # except it stores the result in a queue instead of a global\n",
    "    def create_interface(self):\n",
    "        mic_transcribe = gr.Interface(\n",
    "            fn=lambda x: self.store_transcription(transcribe_speech(x)),\n",
    "            inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "            outputs=gr.Textbox(label=\"Transcription\")\n",
    "        )\n",
    "        self.interface = gr.Blocks()\n",
    "        with self.interface:\n",
    "            gr.TabbedInterface(\n",
    "                [mic_transcribe],\n",
    "                [\"Transcribe Microphone\"]\n",
    "            )\n",
    "        return self.interface\n",
    "\n",
    "    # we launch the transcription interface\n",
    "    async def get_transcription(self):\n",
    "        self.interface = self.create_interface()\n",
    "        self.interface.launch(\n",
    "            share=False,\n",
    "            server_port=8000, \n",
    "            prevent_thread_lock=True\n",
    "        )\n",
    "\n",
    "        # we poll every 1.5 seconds waiting for something to end up in the queue\n",
    "        while True:\n",
    "            if not self.transcription_queue.empty():\n",
    "                result = self.transcription_queue.get()\n",
    "                if self.interface is not None:\n",
    "                    self.interface.close()\n",
    "                return result\n",
    "            await asyncio.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e0042-5feb-4c25-8668-33debef828a7",
   "metadata": {},
   "source": [
    "Now you have a transcription handler, you can use it instead of the keyboard input interface when you're getting human input when you run your workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905709d6-ad52-4514-a75e-2bae2cfbb802",
   "metadata": {
    "height": 402
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id d087eecf-8e6b-4f3a-917b-b402b99960bf\n",
      "Asking question: How would you answer this question about the candidate? <field>First Name</field>\n",
      "Answer was: Sarah\n",
      "Asking question: How would you answer this question about the candidate? <field>Last Name</field>\n",
      "Answer was: Chen\n",
      "Asking question: How would you answer this question about the candidate? <field>Email</field>\n",
      "Answer was: sarah.chen@email.com\n",
      "Asking question: How would you answer this question about the candidate? <field>Phone</field>\n",
      "Answer was: The candidate's phone number is not provided in the available information.\n",
      "Asking question: How would you answer this question about the candidate? <field>Linkedin</field>\n",
      "Answer was: The candidate's LinkedIn profile can be found at linkedin.com/in/sarahchen.\n",
      "Asking question: How would you answer this question about the candidate? <field>Project Portfolio</field>\n",
      "Answer was: The candidate has a project portfolio that includes notable projects such as EcoTrack, a full-stack application for tracking carbon footprint, which utilizes React, Node.js, and MongoDB, and features a machine learning algorithm for personalized sustainability recommendations. This project was recognized in TechCrunch's \"Top 10 Environmental Impact Apps of 2023.\" Another project is ChatFlow, a real-time chat application developed using the WebSocket protocol and React, which includes end-to-end encryption and message persistence, serving over 5000 monthly active users.\n",
      "Asking question: How would you answer this question about the candidate? <field>Degree</field>\n",
      "Answer was: Bachelor of Science in Computer Science\n",
      "Asking question: How would you answer this question about the candidate? <field>Graduation Date</field>\n",
      "Answer was: The graduation date for the candidate is May 2017.\n",
      "Asking question: How would you answer this question about the candidate? <field>Current Job title</field>\n",
      "Answer was: Senior Full Stack Developer\n",
      "Asking question: How would you answer this question about the candidate? <field>Current Employer</field>\n",
      "Answer was: TechFlow Solutions\n",
      "Asking question: How would you answer this question about the candidate? <field>Technical Skills</field>\n",
      "Answer was: The candidate possesses a diverse set of technical skills across both frontend and backend development. On the frontend, they are proficient in React.js, Redux, Next.js, TypeScript, Vue.js, and Nuxt.js, along with HTML5, CSS3, and SASS/SCSS. They also have experience with testing frameworks like Jest and React Testing Library, as well as build tools such as WebPack and Babel. \n",
      "\n",
      "On the backend, their skills include Node.js, Express.js, Python, and Django, with expertise in GraphQL and REST APIs. They are familiar with databases like PostgreSQL and MongoDB. This combination of skills indicates a strong capability in full stack development, enabling them to handle both client-side and server-side programming effectively.\n",
      "Asking question: How would you answer this question about the candidate? <field>Describe why youâ€™re a good fit for this position</field>\n",
      "Answer was: The candidate is a highly skilled Full Stack Web Developer with over 6 years of experience, specializing in technologies such as React and Node.js, which are essential for the position. They have a proven track record of leading technical teams and successfully implementing scalable web applications and microservices, demonstrating their ability to handle complex projects. Their experience in optimizing performance, establishing coding standards, and improving code quality aligns well with the requirements of the role. Additionally, their passion for clean code and mentoring junior developers indicates a commitment to fostering a collaborative and productive work environment. With certifications in cloud architecture and a strong background in both frontend and backend development, the candidate possesses the technical expertise and leadership qualities that make them an excellent fit for the position.\n",
      "Asking question: How would you answer this question about the candidate? <field>Do you have 5 years of experience in React?</field>\n",
      "Answer was: Yes, the candidate has over 6 years of experience as a Full Stack Web Developer, with specific expertise in React.\n",
      "* Running on local URL:  https://0.0.0.0:8000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://s172-29-96-78p8000.lab-aws-production.deeplearning.ai/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = RAGWorkflow(timeout=600, verbose=False)\n",
    "\n",
    "handler = w.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    application_form=\"./data/fake_application_form.pdf\"\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "  if isinstance(event, InputRequiredEvent):\n",
    "      # Get transcription\n",
    "      transcription_handler = TranscriptionHandler()\n",
    "      response = await transcription_handler.get_transcription()\n",
    "\n",
    "      handler.ctx.send_event(\n",
    "          HumanResponseEvent(\n",
    "              response=response\n",
    "          )\n",
    "      )\n",
    "\n",
    "response = await handler\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fff23-cad0-4690-b166-2cc399f3e0c3",
   "metadata": {},
   "source": [
    "## Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb621327-73da-4946-a546-cd80732a03ae",
   "metadata": {},
   "source": [
    "You've successfully created an AI agent that responds to spoken feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cecca",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98a735",
   "metadata": {},
   "source": [
    "To learn more about agentic document workflows, you check this [article](https://www.llamaindex.ai/blog/introducing-agentic-document-workflows) and theses [example implementations](https://github.com/run-llama/llamacloud-demo/tree/main/examples/document_workflows)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
