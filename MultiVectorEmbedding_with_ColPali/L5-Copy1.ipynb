{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9b750c",
   "metadata": {},
   "source": [
    "# L5 - Building Multi-modal RAG with ColPali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29608a64",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5793d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> üìí &nbsp; For more help, please see the <em>\"Appendix ‚Äì Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419bf6f",
   "metadata": {},
   "source": [
    "The following cell is not in the video and just ensures output later in this notebook will render properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2015790edf79186",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518bba0b10f874f0",
   "metadata": {
    "height": 164
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1796a",
   "metadata": {},
   "source": [
    "#### Loading MUVERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ao1uve5a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:18:51.904613Z",
     "start_time": "2025-11-17T22:18:51.606368Z"
    },
    "height": 198
   },
   "outputs": [],
   "source": [
    "from fastembed.postprocess.muvera import Muvera\n",
    "\n",
    "# Create MUVERA instance with same config from Lesson 4\n",
    "muvera = Muvera(\n",
    "    dim=128,  # ColPali embedding dimensionality\n",
    "    k_sim=6,  # 64 clusters (2^6)\n",
    "    dim_proj=16,  # Compress to 16 dimensions per cluster\n",
    "    r_reps=20,  # 20 repetitions\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681566ff",
   "metadata": {},
   "source": [
    "#### Creating Qdrant Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34ff21c6d07ae3",
   "metadata": {
    "height": 266
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Upserting embeddings: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Expect this cell may take several minutes to finish\n",
    "from qdrant_client import QdrantClient, models\n",
    "from helper import recreate_colpali_optimizations_collection\n",
    "\n",
    "# Collection from Lesson 3 + MUVERA\n",
    "collection_name = \"colpali-optimizations\"\n",
    "\n",
    "# Connect to Qdrant\n",
    "qdrant = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "# Recreate the collection with all optimizations including MUVERA\n",
    "recreate_colpali_optimizations_collection(\n",
    "    qdrant, collection_name, muvera=muvera\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005ece2",
   "metadata": {},
   "source": [
    "#### Loading Precomputed Embeddings of Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ad51dffd06803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:05.318719Z",
     "start_time": "2025-11-17T22:20:05.312384Z"
    },
    "height": 504
   },
   "outputs": [],
   "source": [
    "from helper import load_or_compute_rag_query_embeddings\n",
    "import numpy as np\n",
    "\n",
    "# Load precomputed RAG query embeddings\n",
    "rag_queries_df = load_or_compute_rag_query_embeddings(load_precomputed=True)\n",
    "\n",
    "\n",
    "def embed_query(query_text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a text query to ColPali embedding using precomputed vectors.\n",
    "    \n",
    "    Only supports the precomputed RAG queries. Raises an error for unknown queries.\n",
    "    \"\"\"\n",
    "    # Look up query in the precomputed embeddings DataFrame\n",
    "    matching_rows = rag_queries_df[rag_queries_df[\"query\"] == query_text]\n",
    "    \n",
    "    if len(matching_rows) == 0:\n",
    "        # Query not found in precomputed embeddings\n",
    "        available_queries = rag_queries_df[\"query\"].tolist()\n",
    "        raise ValueError(\n",
    "            f\"Query not found in precomputed embeddings.\\n\"\n",
    "            f\"Query: '{query_text}'\\n\"\n",
    "            f\"Available queries:\\n\" + \n",
    "            \"\\n\".join(f\"  - {q}\" for q in available_queries)\n",
    "        )\n",
    "    \n",
    "    # Return the precomputed embedding as NumPy array\n",
    "    return np.stack(matching_rows.iloc[0][\"query_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bdc137ab975b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:05.367233Z",
     "start_time": "2025-11-17T22:20:05.361935Z"
    },
    "height": 79
   },
   "outputs": [],
   "source": [
    "test_query = \"Describe the concept of the 'one learning algorithm'\"\n",
    "test_embedding = embed_query(test_query)\n",
    "test_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e3e1c",
   "metadata": {},
   "source": [
    "#### Creating Retrieval Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de766adc4a6f3a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:05.484522Z",
     "start_time": "2025-11-17T22:20:05.481808Z"
    },
    "height": 368
   },
   "outputs": [],
   "source": [
    "def retrieve(\n",
    "    query_text: str, using: str, top_k: int = 3\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using specified optimization method.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_query(query_text)\n",
    "    results = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding,\n",
    "        using=using,\n",
    "        search_params=models.SearchParams(\n",
    "            quantization=models.QuantizationSearchParams(rescore=False)\n",
    "        ),\n",
    "        limit=top_k,\n",
    "    )\n",
    "    return [\n",
    "        (point.payload[\"image_path\"], point.score)\n",
    "        for point in results.points\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4e93f32db4ca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:05.595318Z",
     "start_time": "2025-11-17T22:20:05.591809Z"
    },
    "height": 572
   },
   "outputs": [],
   "source": [
    "def retrieve_with_two_stage(\n",
    "    query_text: str,\n",
    "    top_k: int = 3,\n",
    "    prefetch_multiplier: int = 5,\n",
    "    rerank_using: str = \"original\",\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Two-stage retrieval: MUVERA for fast candidates, flexible reranking.\n",
    "    \"\"\"\n",
    "    # Embed query with both ColPali and MUVERA\n",
    "    query_colpali = embed_query(query_text)\n",
    "    query_muvera = muvera.process_query(query_colpali)\n",
    "\n",
    "    # Single API call with two-stage retrieval\n",
    "    results = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=query_muvera,\n",
    "                using=\"muvera_fde\",\n",
    "                limit=top_k * prefetch_multiplier,\n",
    "            )\n",
    "        ],\n",
    "        query=query_colpali,\n",
    "        using=rerank_using,\n",
    "        limit=top_k,\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        (point.payload[\"image_path\"], point.score)\n",
    "        for point in results.points\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0830e76898b591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:07.196965Z",
     "start_time": "2025-11-17T22:20:05.642506Z"
    },
    "height": 181
   },
   "outputs": [],
   "source": [
    "from helper import display_retrieved_documents\n",
    "\n",
    "# Test the original ColPali retriever\n",
    "test_query = \"How does the human brain work?\"\n",
    "results = retrieve(test_query, using=\"original\", top_k=3)\n",
    "\n",
    "# Display the retrieved images\n",
    "fig = display_retrieved_documents(results)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fdfa3",
   "metadata": {},
   "source": [
    "#### RAG Pipeline Helper Function\n",
    "Performs retrieval and provides retrieved document pages to VLM to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:07.287130Z",
     "start_time": "2025-11-17T22:20:07.282989Z"
    },
    "height": 1082
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from helper import pil_image_to_base64\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    query_text: str,\n",
    "    image_paths: list[str],\n",
    "    model: str = \"gpt-4o\",\n",
    "    max_tokens: int = 500,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using OpenAI vision model.\n",
    "    \"\"\"\n",
    "    # Build the messages array\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant that \"\n",
    "                \"answers questions based on the \"\n",
    "                \"provided document images. \"\n",
    "                \"Read the images carefully and \"\n",
    "                \"provide accurate answers. \"\n",
    "                \"Answer in Markdown and highlight \"\n",
    "                \"the most important parts\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Build user message with text and images\n",
    "    user_content = [{\"type\": \"text\", \"text\": query_text}]\n",
    "\n",
    "    # Add each image (up to OpenAI's limit of 10)\n",
    "    for image_path in image_paths[:10]:\n",
    "        # Load and encode image\n",
    "        img = Image.open(image_path)\n",
    "        base64_img = pil_image_to_base64(img)\n",
    "\n",
    "        user_content.append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": (f\"data:image/png;base64,{base64_img}\"),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Call OpenAI API\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5028b",
   "metadata": {},
   "source": [
    "#### Demonstrating a Simple RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-generation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:15.785667Z",
     "start_time": "2025-11-17T22:20:07.331247Z"
    },
    "height": 98
   },
   "outputs": [],
   "source": [
    "image_paths = [path for path, _ in results]\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "answer = generate_answer(test_query, image_paths)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-queries",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:15.839364Z",
     "start_time": "2025-11-17T22:20:15.837437Z"
    },
    "height": 113
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Describe the concept of the 'one learning algorithm'\",\n",
    "    \"Explain the size vs performance tradeoff\",\n",
    "    \"What was the coffee mug example used to present?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-execution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:45.884678Z",
     "start_time": "2025-11-17T22:20:15.883862Z"
    },
    "height": 387
   },
   "outputs": [],
   "source": [
    "from helper import display_retrieved_documents\n",
    "\n",
    "for query in queries:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Retrieve documents using original ColPali\n",
    "    results = retrieve(query, using=\"original\", top_k=3)\n",
    "\n",
    "    # Extract image paths\n",
    "    image_paths = [path for path, _ in results]\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generate_answer(query, image_paths)\n",
    "    display(Markdown(answer))\n",
    "    print()\n",
    "\n",
    "    # Visualize the retrieved documents\n",
    "    fig = display_retrieved_documents(results)\n",
    "    fig.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a4c85",
   "metadata": {},
   "source": [
    "#### Testing RAG Pipeline Using Different Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a038e9-2815-4618-a127-3d987e0fd115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:20:45.967531Z",
     "start_time": "2025-11-17T22:20:45.965317Z"
    },
    "height": 183
   },
   "outputs": [],
   "source": [
    "# Map optimization names to their vector names\n",
    "retrievers = [\n",
    "    (\"Original ColPali\", \"original\"),\n",
    "    # (\"Scalar Quantized\", \"scalar_quantized\"),\n",
    "    (\"Binary Quantized\", \"binary_quantized\"),\n",
    "    (\"Hierarchical 2x\", \"hierarchical_2x\"),\n",
    "    # (\"Hierarchical 4x\", \"hierarchical_4x\"),\n",
    "    # (\"Row Pooled\", \"row_pooled\"),\n",
    "    # (\"Column Pooled\", \"column_pooled\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-comparison",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:21:08.234208Z",
     "start_time": "2025-11-17T22:20:46.011948Z"
    },
    "height": 302
   },
   "outputs": [],
   "source": [
    "for retriever_name, using in retrievers:\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Retrieve documents using specified optimization\n",
    "    results = retrieve(test_query, using=using, top_k=3)\n",
    "\n",
    "    # Extract image paths\n",
    "    image_paths = [path for path, _ in results]\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generate_answer(test_query, image_paths)\n",
    "    display(Markdown(answer))\n",
    "\n",
    "    fig = display_retrieved_documents(results)\n",
    "    fig.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e57697",
   "metadata": {},
   "source": [
    "#### Comparing Two-Stage Retrieval with ColPali in the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z7ynuo00mze",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:21:28.450348Z",
     "start_time": "2025-11-17T22:21:08.323080Z"
    },
    "height": 523
   },
   "outputs": [],
   "source": [
    "# Compare two-stage retrieval with original ColPali\n",
    "comparison_retrievers = [\n",
    "    (\n",
    "        \"Original ColPali\",\n",
    "        retrieve(test_query, using=\"original\", top_k=3),\n",
    "    ),\n",
    "    (\n",
    "        \"Two-Stage (MUVERA + ColPali)\",\n",
    "        retrieve_with_two_stage(\n",
    "            test_query, rerank_using=\"original\", top_k=3\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for retriever_name, results in comparison_retrievers:\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Extract image paths\n",
    "    image_paths = [path for path, _ in results]\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generate_answer(test_query, image_paths)\n",
    "    display(Markdown(answer))\n",
    "    print()\n",
    "\n",
    "    # Visualize the results\n",
    "    fig = display_retrieved_documents(results)\n",
    "    fig.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad3ab3",
   "metadata": {},
   "source": [
    "#### Testing Different Reranking Strategies with the same MUVERA prefetch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20lscn9jt9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:21:57.545405Z",
     "start_time": "2025-11-17T22:21:28.526487Z"
    },
    "height": 487
   },
   "outputs": [],
   "source": [
    "# Test different reranking strategies with same MUVERA prefetch\n",
    "reranking_strategies = [\n",
    "    (\"Original ColPali\", \"original\"),\n",
    "    (\"Binary Quantized\", \"binary_quantized\"),\n",
    "    (\"Hierarchical 2x\", \"hierarchical_2x\"),\n",
    "]\n",
    "\n",
    "for strategy_name, rerank_using in reranking_strategies:\n",
    "    print(f\"\\nMUVERA + {strategy_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Retrieve documents with specified reranking strategy\n",
    "    results = retrieve_with_two_stage(\n",
    "        test_query, top_k=3, rerank_using=rerank_using\n",
    "    )\n",
    "\n",
    "    # Extract image paths\n",
    "    image_paths = [path for path, _ in results]\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generate_answer(test_query, image_paths)\n",
    "    display(Markdown(answer))\n",
    "    print()\n",
    "\n",
    "    fig = display_retrieved_documents(results)\n",
    "    fig.show()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
