{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e138af4c",
   "metadata": {},
   "source": [
    "# Lesson 10 - Creating an Agentic multi-agent system using A2A with BeeAI Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612cb21-3101-432b-81f9-2ef56e33643e",
   "metadata": {},
   "source": [
    "In this final code lesson, you will create a comprehensive \"Healthcare Concierge\" system. You will use the **BeeAI Framework** to orchestrate all three agents you have built so far (Policy, Research, and Provider). The BeeAI `RequirementAgent` will act as a router, deciding which A2A agent to hand off to based on the user's complex query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681c164",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    %% User / Client Layer\n",
    "    User\n",
    "    \n",
    "    %% Main Orchestrator Layer (Lesson 9)\n",
    "    subgraph OrchestratorLayer [Router/Requirement Agent]\n",
    "        Concierge[\"<b>Healthcare Concierge Agent</b><br/>(BeeAI Framework)<br/><code>Port: 9996</code>\"]\n",
    "    end\n",
    "\n",
    "    subgraph SubAgents [A2A Agent Servers]\n",
    "        direction LR\n",
    "\n",
    "        PolicyAgent[\"<b>Policy Agent</b><br/>(Anthropic Claude with A2A SDK)<br/><code>Port: 9999</code>\"]\n",
    "        ResearchAgent[\"<b>Research Agent</b><br/>(Google ADK)<br/><code>Port: 9998</code>\"]\n",
    "\n",
    "        ProviderAgent[\"<b>Provider Agent</b><br/>(LangGraph + LangChain)<br/><code>Port: 9997</code>\"]\n",
    "    end\n",
    "\n",
    "    %% Data & Tools Layer\n",
    "    subgraph DataLayer [Data Sources & Tools]\n",
    "        PDF[\"Policy PDF\"]\n",
    "        Google[Google Search Tool]\n",
    "        MCPServer[\"FastMCP Server<br/>(doctors.json)\"]\n",
    "    end\n",
    "    \n",
    "    Label_UA[\"Sends Query - A2A\"]\n",
    "    Label_CP[\"A2A\"]\n",
    "    Label_CR[\"A2A\"]\n",
    "    Label_CProv[\"A2A\"]\n",
    "    Label_MCP[\"MCP (stdio)\"]\n",
    "\n",
    "    %% -- CONNECTIONS --\n",
    "    \n",
    "    User --- Label_UA --> Concierge\n",
    "\n",
    "    Concierge <--- Label_CP --> PolicyAgent\n",
    "    Concierge <--- Label_CR --> ResearchAgent\n",
    "    Concierge <--- Label_CProv --> ProviderAgent\n",
    "    \n",
    "    PolicyAgent -- \"Reads\" --> PDF\n",
    "    ResearchAgent -- \"Calls\" --> Google\n",
    "    \n",
    "    ProviderAgent --- Label_MCP --> MCPServer\n",
    "\n",
    "    classDef orchestrator fill:#f9f,stroke:#333,stroke-width:2px;\n",
    "    classDef agent fill:#e1f5fe,stroke:#0277bd,stroke-width:2px;\n",
    "    classDef tool fill:#fff3e0,stroke:#ef6c00,stroke-width:1px,stroke-dasharray: 5 5;\n",
    "    \n",
    "    classDef protocolLabel fill:#ffffff,stroke:none,color:#000;\n",
    "    \n",
    "    class Concierge orchestrator;\n",
    "    class PolicyAgent,ResearchAgent,ProviderAgent agent;\n",
    "    class PDF,Google,MCPServer tool;\n",
    "    \n",
    "    class Label_UA,Label_CP,Label_CR,Label_CProv,Label_MCP protocolLabel;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbb197-2b05-45e3-a5eb-0e9257bcf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from helpers import authenticate\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d393b-91d1-469b-b69d-c16f28d904ee",
   "metadata": {},
   "source": [
    "## 10.1. Start All Agent Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14212237",
   "metadata": {},
   "source": [
    "First, ensure that your Policy Agent is running:\n",
    "- Open Terminal 1 by running the cell below.\n",
    "- If the agent is still running from the previous lesson, you don't need to do anything.\n",
    "- If the agent has stopped, type: `uv run a2a_policy_agent.py` (you don't need to go back to the previous lesson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb88c95-3057-4477-beec-bad987ea7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "url = os.environ.get(\"DLAI_LOCAL_URL\").format(port=8888)\n",
    "# Terminal 1: uv run a2a_policy_agent.py\n",
    "IFrame(f\"{url}terminals/1\", width=550, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9adf2",
   "metadata": {},
   "source": [
    "Second, ensure that your Research Agent is running:\n",
    "- Open Terminal 2 by running the cell below.\n",
    "- If the agent is still running from the previous lesson, you don't need to do anything.\n",
    "- If the agent has stopped, type: `uv run a2a_research_agent.py` (you don't need to go back to the previous lesson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48527c-8584-4011-a954-acd489035842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal 2: uv run a2a_research_agent.py\n",
    "IFrame(f\"{url}terminals/2\", width=550, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555125c",
   "metadata": {},
   "source": [
    "Third, ensure that your Provider Agent is running:\n",
    "- Open Terminal 3 by running the cell below.\n",
    "- If the agent is still running from the previous lesson, you don't need to do anything.\n",
    "- If the agent has stopped, type: `uv run a2a_provider_agent.py` (you don't need to go back to the previous lesson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8d64b-dfcd-481a-9508-5dc3e8ffb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal 3: uv run a2a_provider_agent.py\n",
    "IFrame(f\"{url}terminals/3\", width=550, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a4546-8a64-41e9-8e4b-5078e9363a26",
   "metadata": {},
   "source": [
    "## 10.2. Define BeeAI Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19787f",
   "metadata": {},
   "source": [
    "Here you will:\n",
    "1.  Import BeeAI framework components, including `RequirementAgent` and `HandoffTool`.\n",
    "2.  Define `A2AAgent` instances for each of your running servers.\n",
    "3.  Use `check_agent_exists()` to fetch the metadata (AgentCard) from each server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff7b46-1481-48cc-a7c7-6544ba3f01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.adapters.a2a.agents import A2AAgent\n",
    "from beeai_framework.adapters.vertexai import VertexAIChatModel\n",
    "from beeai_framework.agents.requirement import RequirementAgent\n",
    "from beeai_framework.agents.requirement.requirements.conditional import (\n",
    "    ConditionalRequirement,\n",
    ")\n",
    "from beeai_framework.memory import UnconstrainedMemory\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "from beeai_framework.middleware.trajectory import EventMeta, GlobalTrajectoryMiddleware\n",
    "from beeai_framework.tools import Tool\n",
    "from beeai_framework.tools.handoff import HandoffTool\n",
    "from beeai_framework.tools.think import ThinkTool\n",
    "\n",
    "\n",
    "class ConciseGlobalTrajectoryMiddleware(GlobalTrajectoryMiddleware):\n",
    "    def _format_prefix(self, meta: EventMeta) -> str:\n",
    "        prefix = super()._format_prefix(meta)\n",
    "        return prefix.rstrip(\": \")\n",
    "\n",
    "    def _format_payload(self, value: Any) -> str:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bf7f8-b214-4da7-9828-87b465d50530",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "_, project_id = authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f15c5-9150-466b-9b0a-af5fb7a2040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.environ.get(\"AGENT_HOST\")\n",
    "policy_agent_port = os.environ.get(\"POLICY_AGENT_PORT\")\n",
    "research_agent_port = os.environ.get(\"RESEARCH_AGENT_PORT\")\n",
    "provider_agent_port = os.environ.get(\"PROVIDER_AGENT_PORT\")\n",
    "healthcare_agent_port = int(os.environ.get(\"HEALTHCARE_AGENT_PORT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a115368-8853-4248-a6d7-dc98e76051ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{policy_agent_port}\", \n",
    "    memory=UnconstrainedMemory()\n",
    ")\n",
    "# Run `check_agent_exists()` to fetch and populate AgentCard\n",
    "asyncio.run(policy_agent.check_agent_exists())\n",
    "print(\"\\tℹ️\", f\"{policy_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42619d7-24e0-402d-9535-243eeae5e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{research_agent_port}\", \n",
    "    memory=UnconstrainedMemory()\n",
    ")\n",
    "asyncio.run(research_agent.check_agent_exists())\n",
    "print(\"\\tℹ️\", f\"{research_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0895c4-c3fd-4782-a330-1124daf42d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_agent = A2AAgent(\n",
    "    url=f\"http://{host}:{provider_agent_port}\", \n",
    "    memory=UnconstrainedMemory()\n",
    ")\n",
    "asyncio.run(provider_agent.check_agent_exists())\n",
    "print(\"\\tℹ️\", f\"{provider_agent.name} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb63784-438b-409c-bff6-25d67b2b5a5d",
   "metadata": {},
   "source": [
    "## 10.3. Configure the Orchestrator (Healthcare Concierge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f691f",
   "metadata": {},
   "source": [
    "You will now configure the `RequirementAgent`. This agent uses a `VertexAIChatModel` and is equipped with `HandoffTool`s connected to your A2A agents. The instructions explicitly guide the LLM on how to use each specific agent (Research for conditions, Policy for insurance, Provider for doctors) to answer multi-part questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac7ebf-ad87-4dd3-b5ac-7a869d23cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthcare_agent = RequirementAgent(\n",
    "    name=\"Healthcare Agent\",\n",
    "    description=\"\"\"A personal concierge for Healthcare Information, \n",
    "    customized to your policy.\"\"\",\n",
    "    llm=VertexAIChatModel(\n",
    "        model_id=\"gemini-2.5-flash\",\n",
    "        project=project_id,\n",
    "        location=\"global\",\n",
    "        allow_parallel_tool_calls=True,\n",
    "        allow_prompt_caching=False,\n",
    "        settings={\n",
    "            \"api_base\": f\"{os.getenv('GOOGLE_VERTEX_BASE_URL')}\",\n",
    "            \"use_psc_endpoint_format\": True,\n",
    "        }\n",
    "    ),\n",
    "    tools=[\n",
    "        thinktool := ThinkTool(),\n",
    "        policy_tool := HandoffTool(\n",
    "            target=policy_agent,\n",
    "            name=policy_agent.name,\n",
    "            description=policy_agent.agent_card.description,\n",
    "        ),\n",
    "        research_tool := HandoffTool(\n",
    "            target=research_agent,\n",
    "            name=research_agent.name,\n",
    "            description=research_agent.agent_card.description,\n",
    "        ),\n",
    "        provider_tool := HandoffTool(\n",
    "            target=provider_agent,\n",
    "            name=provider_agent.name,\n",
    "            description=provider_agent.agent_card.description,\n",
    "        ),\n",
    "    ],\n",
    "    requirements=[\n",
    "        # ConditionalRequirement(policy_tool, consecutive_allowed=False),\n",
    "        ConditionalRequirement(\n",
    "            thinktool, force_at_step=1, force_after=Tool, \n",
    "            consecutive_allowed=False\n",
    "        ),\n",
    "    ],\n",
    "    role=\"Healthcare Concierge\",\n",
    "    instructions=(\n",
    "        f\"\"\"You are a concierge for healthcare services. Your task is \n",
    "        to handoff to one or more agents to answer questions and provide \n",
    "        a detailed summary of their answers. Be sure that all of their \n",
    "        questions are answered before responding.\n",
    "        Use `{policy_agent.name}` to answer insurance-related questions.\n",
    "        \n",
    "        IMPORTANT: When returning answers about providers, only output \n",
    "        providers from `{provider_agent.name}` and only provide insurance \n",
    "        information based on the results from `{policy_agent.name}`.\n",
    "\n",
    "        In your output, put which agent gave you the information!\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\tℹ️\", f\"{healthcare_agent.meta.name} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6abb07-34c9-4e21-b18a-54e9721467fe",
   "metadata": {},
   "source": [
    "## 10.4. Run the Full Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223bf0c",
   "metadata": {},
   "source": [
    "Test the system with a complex query that requires information from all three sub-agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afacd17-fa19-4745-ab26-6f8e3c746e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await healthcare_agent.run(\n",
    "    \"\"\"I'm based in Austin, TX. How do I get mental health therapy near me \n",
    "    and what does my insurance cover?\"\"\"\n",
    ").middleware(ConciseGlobalTrajectoryMiddleware())\n",
    "display(Markdown(response.last_message.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393b196-37ce-4227-a3e0-031a63a307c1",
   "metadata": {},
   "source": [
    "## 10.5. Write the Agent Code to a File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cba5b2-09cc-4ad9-9870-9383b5e43c86",
   "metadata": {},
   "source": [
    "Write the Concierge agent code to a Python file to be able to run it as an A2A Agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a753f13-c118-426f-9c06-8e6c69045f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../a2a_healthcare_agent.py\n",
    "from dotenv import load_dotenv\n",
    "from helpers import authenticate\n",
    "from typing import Any\n",
    "import asyncio\n",
    "import os\n",
    "from beeai_framework.adapters.a2a.serve.server import A2AServer, A2AServerConfig\n",
    "from beeai_framework.adapters.a2a.agents import A2AAgent\n",
    "from beeai_framework.adapters.vertexai import VertexAIChatModel\n",
    "from beeai_framework.agents.requirement import RequirementAgent\n",
    "from beeai_framework.agents.requirement.requirements.conditional import ConditionalRequirement\n",
    "from beeai_framework.memory import UnconstrainedMemory\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "from beeai_framework.middleware.trajectory import EventMeta, GlobalTrajectoryMiddleware\n",
    "from beeai_framework.serve.utils import LRUMemoryManager\n",
    "from beeai_framework.tools import Tool, tool\n",
    "from beeai_framework.tools.handoff import HandoffTool\n",
    "from beeai_framework.tools.think import ThinkTool\n",
    "\n",
    "# Log only tool calls\n",
    "class ConciseGlobalTrajectoryMiddleware(GlobalTrajectoryMiddleware):\n",
    "    def _format_prefix(self, meta: EventMeta) -> str:\n",
    "        prefix = super()._format_prefix(meta)\n",
    "        return prefix.rstrip(\": \")\n",
    "\n",
    "    def _format_payload(self, value: Any) -> str:\n",
    "        return \"\"\n",
    "\n",
    "def main():\n",
    "    print(f\"Running A2A Orchestrator Agent\")\n",
    "    load_dotenv()\n",
    "    _, project_id = authenticate()\n",
    "\n",
    "    host = os.environ.get(\"AGENT_HOST\")\n",
    "    policy_agent_port = os.environ.get(\"POLICY_AGENT_PORT\")\n",
    "    research_agent_port = os.environ.get(\"RESEARCH_AGENT_PORT\")\n",
    "    provider_agent_port = os.environ.get(\"PROVIDER_AGENT_PORT\")\n",
    "    healthcare_agent_port = int(os.environ.get(\"HEALTHCARE_AGENT_PORT\"))\n",
    "\n",
    "    # Log only tool calls\n",
    "    GlobalTrajectoryMiddleware(target=[Tool]) \n",
    "\n",
    "    policy_agent = A2AAgent(\n",
    "        url=f\"http://{host}:{policy_agent_port}\", memory=UnconstrainedMemory()\n",
    "    )\n",
    "    # Run `check_agent_exists()` to fetch and populate AgentCard\n",
    "    asyncio.run(policy_agent.check_agent_exists())\n",
    "    print(\"\\tℹ️\", f\"{policy_agent.name} initialized\")\n",
    "    \n",
    "    research_agent = A2AAgent(\n",
    "        url=f\"http://{host}:{research_agent_port}\", memory=UnconstrainedMemory()\n",
    "    )\n",
    "    asyncio.run(research_agent.check_agent_exists())\n",
    "    print(\"\\tℹ️\", f\"{research_agent.name} initialized\")\n",
    "\n",
    "    provider_agent = A2AAgent(\n",
    "        url=f\"http://{host}:{provider_agent_port}\", memory=UnconstrainedMemory()\n",
    "    )\n",
    "    asyncio.run(provider_agent.check_agent_exists())\n",
    "    print(\"\\tℹ️\", f\"{provider_agent.name} initialized\")\n",
    "\n",
    "    healthcare_agent = RequirementAgent(\n",
    "        name=\"Healthcare Agent\",\n",
    "        description=\"A personal concierge for Healthcare Information, customized to your policy.\",\n",
    "        llm=VertexAIChatModel(\n",
    "            model_id=\"gemini-2.5-flash\",\n",
    "            project=project_id,\n",
    "            location=\"global\",\n",
    "            allow_parallel_tool_calls=True,\n",
    "            settings={\n",
    "                \"api_base\": f\"{os.getenv('GOOGLE_VERTEX_BASE_URL')}\",\n",
    "                \"use_psc_endpoint_format\": True,\n",
    "            }\n",
    "        ),\n",
    "        tools=[\n",
    "            thinktool:=ThinkTool(),\n",
    "            policy_tool:=HandoffTool(\n",
    "                target=policy_agent,\n",
    "                name=policy_agent.name,\n",
    "                description=policy_agent.agent_card.description,\n",
    "            ),\n",
    "            research_tool:=HandoffTool(\n",
    "                target=research_agent,\n",
    "                name=research_agent.name,\n",
    "                description=research_agent.agent_card.description,\n",
    "            ),\n",
    "            provider_tool:=HandoffTool(\n",
    "                target=provider_agent,\n",
    "                name=provider_agent.name,\n",
    "                description=provider_agent.agent_card.description,\n",
    "            ),\n",
    "        ],\n",
    "        requirements=[\n",
    "            ConditionalRequirement(policy_tool, consecutive_allowed=False),\n",
    "            ConditionalRequirement(thinktool, force_at_step=1, force_after=Tool, consecutive_allowed=False),\n",
    "        ],\n",
    "        role=\"Healthcare Concierge\",\n",
    "        instructions=(\n",
    "            f\"\"\"You are a concierge for healthcare services. Your task is to handoff to one or more agents to answer questions and provide a detailed summary of their answers. Be sure that all of their questions are answered before responding.\n",
    "            Use `{policy_agent.name}` to answer insurance-related questions.\n",
    "            \n",
    "            IMPORTANT: When returning answers about providers, only output providers from `{provider_agent.name}` and only provide insurance information based on the results from `{policy_agent.name}`.\n",
    "    \n",
    "            In your output, put which agent gave you the information!\"\"\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(\"\\tℹ️\", f\"{healthcare_agent.meta.name} initialized\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab28204-b0aa-4299-9ca8-6d494c1dd99c",
   "metadata": {},
   "source": [
    "### Add the A2AServer registration\n",
    "The only change to run an agent as an A2A agent is to add the single A2AServer registration statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d46af6-ff74-4f74-9e92-afd2603f8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../a2a_healthcare_agent.py -a\n",
    "\n",
    "    # Register the agent with the A2A server and run the HTTP server\n",
    "    # we use LRU memory manager to keep limited amount of sessions in the memory\n",
    "    A2AServer(\n",
    "        config=A2AServerConfig(port=healthcare_agent_port, protocol=\"jsonrpc\", host=host ),\n",
    "        memory_manager=LRUMemoryManager(maxsize=100),\n",
    "    ).register(healthcare_agent, send_trajectory=True).serve()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3aaec-796b-4373-9a92-0e694eb932c0",
   "metadata": {},
   "source": [
    "## 10.6. Serve the Concierge Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0d12c",
   "metadata": {},
   "source": [
    "Finally, you can register this high-level \"Concierge\" agent itself as an A2A server. This demonstrates the recursive power of A2A: an agent composed of other A2A agents can itself be exposed as an A2A agent.\n",
    "\n",
    "Now to activate your configured A2A agent, you would need to run your agent server. You can run the agent server using `uv`:\n",
    "\n",
    "- Open Terminal 4 by running the cell below\n",
    "- Type `uv run a2a_healthcare_agent.py` to run the server and activate your A2A agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "url = os.environ.get(\"DLAI_LOCAL_URL\").format(port=8888)\n",
    "IFrame(f\"{url}terminals/4\", width=550, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03987063-257b-4c3f-b873-0fb7eb8d1407",
   "metadata": {},
   "source": [
    "## 10.7. Run the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8e929-a710-44bd-8a05-91a68f053b21",
   "metadata": {},
   "source": [
    "Question: I'm based in Austin, TX. How do I get mental health therapy near me and what does my insurance cover?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba19a50-8d6b-4fb0-b560-770f06e166b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2AAgent(url=\"http://127.0.0.1:9996\", \n",
    "                 memory=UnconstrainedMemory())\n",
    "response = await agent.run(\n",
    "    \"I'm based in Austin, TX. How do I get mental health therapy near me and what does my insurance cover?\"\n",
    ").middleware(ConciseGlobalTrajectoryMiddleware())\n",
    "display(Markdown(response.last_message.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0308014-75d1-4d2c-b7f9-7d95caf73d49",
   "metadata": {},
   "source": [
    "## 10.8. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b485fe",
   "metadata": {},
   "source": [
    "- [BeeAI Framework](https://framework.beeai.dev/introduction/welcome)\n",
    "- [BeeAI Requirement Agent](https://framework.beeai.dev/modules/agents/requirement-agent)\n",
    "- [BeeAI Framework GitHub](https://github.com/i-am-bee/beeai-framework)\n",
    "- [Equivalent notebook in the course repo](https://github.com/holtskinner/A2AWalkthrough/blob/main/8_BeeAIRequirement.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd4d7c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ⬇ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download\"</em>.</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
