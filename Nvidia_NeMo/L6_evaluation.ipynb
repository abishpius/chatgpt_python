{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cc9b57",
   "metadata": {},
   "source": [
    "# Lesson 6: Evaluation - Finding and Fixing Bugs with NAT Eval\n",
    "\n",
    "In previous lessons, you built, traced, and integrated agents. But how do you know if they actually work? Manual testing catches obvious errors, but subtle bugs hide in edge cases. A tool might work 80% of the time and fail silently the other 20%.\n",
    "\n",
    "In this lesson, you'll create evaluation datasets with ground truth answers, run systematic tests, discover a hidden bug in your climate agent, fix it, and verify the improvement. This transforms agent development from manual testing into a data-driven engineering process.\n",
    "<div style=\"background-color: #e7f3fe; border-left: 6px solid #2196F3; padding: 15px; margin: 10px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">üéØ Learning Objectives</h4>\n",
    "By the end of this lesson, you'll know how to:\n",
    "<ul>\n",
    "<li>Create evaluation datasets with ground truth answers</li>\n",
    "<li>Run systematic tests to discover unexpected agent behaviors</li>\n",
    "<li>Use evaluation results to identify and fix bugs</li>\n",
    "<li>Verify improvements with before/after comparisons</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d64e281-3b7c-47ac-9b05-8f4cfe177292",
   "metadata": {
    "height": 130
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key set: Yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify it loaded\n",
    "print(\"API key set:\", \"Yes\" if os.getenv('NVIDIA_API_KEY') else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d9b018",
   "metadata": {
    "height": 64,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the climate analyzer package\n",
    "!cd climate_analyzer && pip install -e . && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4394ab1",
   "metadata": {},
   "source": [
    "## Evaluation Dataset\n",
    "An evaluation dataset consists of questions paired with ground truth answers. The agent's responses are compared against these known-correct answers to calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f60f4",
   "metadata": {
    "height": 181
   },
   "outputs": [],
   "source": [
    "# %load climate_analyzer/data/simple_eval.json\n",
    "[\n",
    "  {\n",
    "    \"id\": \"austria_1980\",\n",
    "    \"question\": \"What was the average temperature in Austria in 1980? Please provide the numerical value.\",\n",
    "    \"answer\": \"The average temperature in Austria in 1980 was 6.80\\u00b0C\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66952e5a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f3e5f5; border-left: 6px solid #9c27b0; padding: 15px; margin: 15px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">üí° Evaluation Dataset Structure</h4>\n",
    "Each test case contains:\n",
    "<ul>\n",
    "<li><strong>user_input</strong> - The question to ask the agent</li>\n",
    "<li><strong>reference</strong> - The ground truth answer (what the agent should return)</li>\n",
    "<li><strong>metadata</strong> - Additional context (like expected tool calls)</li>\n",
    "</ul>\n",
    "<br>\n",
    "<strong>Example:</strong>\n",
    "<pre style=\"background-color: #f5f5f5; padding: 10px; border-radius: 3px; margin: 10px 0;\">\n",
    "{\n",
    "  \"user_input\": \"What was Austria's average temperature in 1980?\",\n",
    "  \"reference\": \"6.80¬∞C\"\n",
    "}\n",
    "</pre>\n",
    "</div>\n",
    "\n",
    "### Verify Ground Truth\n",
    "Before evaluating your agent, verify your ground truth answers are actually correct. Otherwise, you're testing against wrong answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8cc246",
   "metadata": {
    "height": 45,
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AU000005010,1980,AU,Austria,48.05,14.1331,KREMSMUENSTER,7.994166666666666\r\n",
      "AUXLT782426,1980,AU,Austria,47.0,15.4333,GRAZ_THALERHOF,7.631666666666667\r\n",
      "AUXLT891651,1980,AU,Austria,47.383,13.456,RADSTADT,4.766666666666667\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"^[^,]*,1980,[^,]*,Austria\" ../resources/climate_data/temperature_annual.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ad2cd-1998-435a-8c53-1f19c2c325b1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; border: 1px solid #ddd; padding: 15px; border-radius: 5px; margin: 15px 0; font-family: monospace;\">\n",
    "<strong>Raw Data for Austria 1980:</strong>\n",
    "<pre style=\"margin: 10px 0; white-space: pre-wrap;\">\n",
    "Austria,1980,01,7.994166666666666\n",
    "Austria,1980,02,7.631666666666667\n",
    "Austria,1980,03,4.766666666666667\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8a8d5c",
   "metadata": {
    "height": 96,
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average temperature for Austria in 1980: 6.80¬∞C\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average to confirm our ground truth:\n",
    "temps = [7.994166666666666, 7.631666666666667, 4.766666666666667]\n",
    "average = sum(temps) / len(temps)\n",
    "print(f\"\\nAverage temperature for Austria in 1980: {average:.2f}¬∞C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7495c85-86af-448b-bee7-7f71f5d91e9a",
   "metadata": {},
   "source": [
    "Add an eval section to your NAT config to define your test dataset and metrics:\n",
    "<div style=\"background-color: #fff3cd; border-left: 6px solid #ffc107; padding: 15px; margin: 15px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">üìã Evaluation Configuration</h4>\n",
    "<pre style=\"background-color: #f5f5f5; padding: 10px; border-radius: 3px; margin: 10px 0;\">\n",
    "eval:\n",
    "  eval_dataset_file_path: data/simple_eval.json  # Test questions + answers\n",
    "  eval_name: simple_test                          # Name for this eval run\n",
    "  eval_output_folder_path: .tmp/nat/climate_analyzer/eval  # Where to save results\n",
    "  eval_metrics:\n",
    "    - _type: answer_accuracy                      # Metric: compare answers\n",
    "      model_name: meta/llama-3.1-70b-instruct    # LLM judges accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8520840",
   "metadata": {
    "height": 1388
   },
   "outputs": [],
   "source": [
    "# %load climate_analyzer/src/climate_analyzer/configs/eval_config.yml\n",
    "llms:\n",
    "  climate_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    base_url: $NVIDIA_BASE_URL \n",
    "    api_key: $NVIDIA_API_KEY\n",
    "    temperature: 0.7\n",
    "    top_p: 0.95\n",
    "    max_tokens: 2048\n",
    "  \n",
    "  calculator_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    base_url: $NVIDIA_BASE_URL\n",
    "    api_key: $NVIDIA_API_KEY\n",
    "    temperature: 0.0\n",
    "    max_tokens: 1024\n",
    "\n",
    "functions:\n",
    "  list_countries:\n",
    "    _type: climate_analyzer/list_countries\n",
    "    description: \"List all available countries in the dataset\"\n",
    "    \n",
    "  calculate_statistics:\n",
    "    _type: climate_analyzer/calculate_statistics\n",
    "    description: \"Calculate temperature statistics globally or for a specific country\"\n",
    "  \n",
    "  filter_by_country:\n",
    "    _type: climate_analyzer/filter_by_country\n",
    "    description: \"Get information about climate data for a specific country\"\n",
    "  \n",
    "  find_extreme_years:\n",
    "    _type: climate_analyzer/find_extreme_years\n",
    "    description: \"Find the warmest or coldest years in the dataset\"\n",
    "  \n",
    "  create_visualization:\n",
    "    _type: climate_analyzer/create_visualization\n",
    "    description: \"Create visualizations including automatic top 5 countries by warming trend (country_comparison plot)\"\n",
    "\n",
    "  station_statistics:\n",
    "    _type: climate_analyzer/station_statistics\n",
    "    description: \"Get statistics on climate stations used in the data\"\n",
    "  \n",
    "  calculator_agent:\n",
    "    _type: climate_analyzer/calculator_agent\n",
    "    description: \"Perform complex mathematical calculations for climate data analysis\"\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  tool_names:\n",
    "    - list_countries\n",
    "    - calculate_statistics\n",
    "    - filter_by_country\n",
    "    - find_extreme_years\n",
    "    - create_visualization\n",
    "    - station_statistics\n",
    "    - calculator_agent\n",
    "  llm_name: climate_llm\n",
    "  max_iterations: 5\n",
    "  parse_agent_response_max_retries: 2\n",
    "  max_tool_calls: 30\n",
    "\n",
    "# Evaluation configuration\n",
    "eval:\n",
    "  general:\n",
    "    output:\n",
    "      dir: ./.tmp/nat/climate_analyzer/eval/simple_test/\n",
    "      cleanup: false  # Keep results for inspection\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: data/simple_eval.json\n",
    "\n",
    "  evaluators:\n",
    "    # Check if the answer is accurate\n",
    "    answer_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: climate_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6d91e",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae76ede",
   "metadata": {},
   "source": [
    "</pre>\n",
    "</div>\n",
    "<div style=\"background-color: #f3e5f5; border-left: 6px solid #9c27b0; padding: 15px; margin: 15px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">üí° How Answer Accuracy Works</h4>\n",
    "<ol>\n",
    "<li>Your agent processes each test question</li>\n",
    "<li>NAT captures the agent's response</li>\n",
    "<li>An LLM judge compares the response to the reference answer</li>\n",
    "<li>The judge assigns a score (0.0 = wrong, 1.0 = correct)</li>\n",
    "<li>NAT calculates average score across all test cases</li>\n",
    "</ol>\n",
    "<br>\n",
    "<strong>Why use an LLM judge?</strong> Exact string matching is too brittle. \"6.80¬∞C\" and \"6.8 degrees Celsius\" are the same answer but different strings. An LLM can judge semantic equivalence.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dfda045",
   "metadata": {
    "height": 45,
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-17 20:11:41 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: src/climate_analyzer/configs/eval_config.yml\n",
      "2025-12-17 20:11:43 - WARNING  - nat.builder.function_info:455 - Using provided input_schema for multi-argument function\n",
      "2025-12-17 20:11:43 - WARNING  - nat.builder.function_info:455 - Using provided input_schema for multi-argument function\n",
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:171: UserWarning: http://jupyter-api-proxy.internal.dlai/rev-proxy/nvidia does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:171: UserWarning: http://jupyter-api-proxy.internal.dlai/rev-proxy/nvidia does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:171: UserWarning: http://jupyter-api-proxy.internal.dlai/rev-proxy/nvidia does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n",
      "Running workflow: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:20<00:00, 20.56s/it]\n",
      "Evaluating Ragas nv_accuracy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "2025-12-17 20:12:06 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-12-17 20:12:06 - INFO     - nat.eval.evaluate:337 - Workflow output written to .tmp/nat/climate_analyzer/eval/simple_test/workflow_output.json\n",
      "2025-12-17 20:12:06 - INFO     - nat.eval.evaluate:348 - Evaluation results written to .tmp/nat/climate_analyzer/eval/simple_test/answer_accuracy_output.json\n",
      "2025-12-17 20:12:06 - INFO     - nat.eval.utils.output_uploader:61 - No S3 config provided; skipping upload.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd climate_analyzer && nat eval --config_file src/climate_analyzer/configs/eval_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c301a-c934-4443-8afc-1599b7549f0f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e7f3fe; border-left: 6px solid #2196F3; padding: 15px; margin: 15px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">üîÑ What's Happening</h4>\n",
    "<ol>\n",
    "<li>NAT loads your test dataset</li>\n",
    "<li>For each test case, it runs your agent with the question</li>\n",
    "<li>Captures the agent's reasoning steps and final answer</li>\n",
    "<li>Sends both the agent's answer and reference answer to the LLM judge</li>\n",
    "<li>Collects scores and saves detailed results to JSON files</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc1636",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc75166-74dc-4fd0-99eb-f920cd786191",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; border: 1px solid #ddd; padding: 15px; border-radius: 5px; margin: 15px 0; font-family: monospace;\">\n",
    "<strong>Generated Files:</strong>\n",
    "<pre style=\"margin: 10px 0; white-space: pre-wrap;\">\n",
    "answer_accuracy_output.json  ‚Üê Detailed results with scores\n",
    "eval_summary.json           ‚Üê High-level metrics\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96717ec",
   "metadata": {},
   "source": [
    "### Score Summary\n",
    "Now you can open the results files and see how your agent performed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "301eead4",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('climate_analyzer/.tmp/nat/climate_analyzer/eval/simple_test/answer_accuracy_output.json', 'r') as f:\n",
    "    answer_accuracy_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18890fa8",
   "metadata": {
    "height": 232
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Results\n",
      "==================================================\n",
      "Average Score: 0.0 / 1.0\n",
      "\n",
      "‚ùì What was the average temperature in Austria in 1980? Please provide the numerical value.\n",
      "‚úÖ Expected: The average temperature in Austria in 1980 was 6.80¬∞C\n",
      "‚ùå Got: The average temperature in Austria in 1980 cannot be determined from the provided statistics.\n",
      "üìà Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìä Evaluation Results\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Average Score: {answer_accuracy_data['average_score']} / 1.0\")\n",
    "print()\n",
    "\n",
    "for item in answer_accuracy_data['eval_output_items']:\n",
    "    r = item['reasoning']\n",
    "    print(f\"‚ùì {r['user_input']}\")\n",
    "    print(f\"‚úÖ Expected: {r['reference']}\")\n",
    "    print(f\"‚ùå Got: {r['response']}\")\n",
    "    print(f\"üìà Score: {item['score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a374f-4326-4fd2-a823-9f833cf2bf71",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffebee; border-left: 6px solid #f44336; padding: 15px; margin: 20px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">‚ùå Initial Results - Something's Wrong</h4>\n",
    "<pre style=\"background-color: white; padding: 10px; border-radius: 3px; margin: 10px 0;\">\n",
    "üìä Evaluation Results\n",
    "==================================================\n",
    "Average Score: 0.0 / 1.0\n",
    "‚ùì What was Austria's average temperature in 1980?\n",
    "‚úÖ Expected: 6.80¬∞C\n",
    "‚ùå Got: 8.08¬∞C\n",
    "üìà Score: 0.0\n",
    "</pre>\n",
    "<br>\n",
    "<strong>The agent got the wrong answer!</strong> Let's investigate why.\n",
    "</div>\n",
    "\n",
    "### Inspect Agent Reasoning\n",
    "Let's examine exactly what the agent did to understand where it went wrong. This code is just parsing the output JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed6448a2",
   "metadata": {
    "height": 963,
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AGENT'S DECISION PROCESS\n",
      "============================================================\n",
      "Question: What was the average temperature in Austria in 1980? Please provide the numerical value.\n",
      "Expected: The average temperature in Austria in 1980 was 6.80¬∞C\n",
      "============================================================\n",
      "\n",
      "**Step 0**\n",
      "üí≠ Thought: To find the average temperature in Austria in 1980, I need to get the temperature statistics for Austria.\n",
      "Action: calculate_statistics\n",
      "Action Input: {\"country\": \"Austria\"}\n",
      "üõ†Ô∏è  Tool: calculate_statistics\n",
      "üì• Input: {\"country\": \"Austria\"}\n",
      "\n",
      "**Step 1**\n",
      "\n",
      "**Step 2**\n",
      "üí≠ Thought: The provided statistics are for the entire period from 1950 to 2025, but I need the average temperature specifically for the year 1980. However, the statistics provided do not include yearly breakdowns, so I cannot determine the exact average temperature for 1980 from this data.\n",
      "Final Answer: The average temperature in Austria in 1980 cannot be determined from the provided statistics.\n",
      "‚úÖ Final Answer: The average temperature in Austria in 1980 cannot be determined from the provided statistics.\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚ùå Actual answer given: The average temperature in Austria in 1980 cannot be determined from the provided statistics.\n",
      "üìä Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Extract the reasoning steps\n",
    "item = answer_accuracy_data['eval_output_items'][0]\n",
    "contexts = item['reasoning']['retrieved_contexts']\n",
    "\n",
    "print(\"ü§ñ AGENT'S DECISION PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {item['reasoning']['user_input']}\")\n",
    "print(f\"Expected: {item['reasoning']['reference']}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Parse each step\n",
    "for i, context in enumerate(contexts):\n",
    "    if context.startswith('**Step'):\n",
    "        # Extract step number and content\n",
    "        lines = context.strip().split('\\n')\n",
    "        step_header = lines[0]\n",
    "        \n",
    "        print(f\"{step_header}\")\n",
    "        \n",
    "        # Look for Thought\n",
    "        if 'Thought:' in context:\n",
    "            thought_start = context.find('Thought:') + 8\n",
    "            thought_end = context.find('\\n\\nAction:') if '\\n\\nAction:' in context else len(context)\n",
    "            thought = context[thought_start:thought_end].strip()\n",
    "            print(f\"üí≠ Thought: {thought}\")\n",
    "        \n",
    "        # Look for Action (tool call)\n",
    "        if 'Action:' in context and 'Action Input:' in context:\n",
    "            action_start = context.find('Action:') + 7\n",
    "            action_end = context.find('\\nAction Input:')\n",
    "            action = context[action_start:action_end].strip()\n",
    "            \n",
    "            input_start = context.find('Action Input:') + 13\n",
    "            input_end = context.find('\\n\\n', input_start) if '\\n\\n' in context[input_start:] else len(context)\n",
    "            action_input = context[input_start:input_end].strip()\n",
    "            \n",
    "            print(f\"üõ†Ô∏è  Tool: {action}\")\n",
    "            print(f\"üì• Input: {action_input}\")\n",
    "        \n",
    "        # Look for tool response (usually JSON)\n",
    "        if i + 1 < len(contexts) and contexts[i + 1].startswith('{'):\n",
    "            print(f\"üì§ Response: {contexts[i + 1][:100]}...\" if len(contexts[i + 1]) > 100 else f\"üì§ Response: {contexts[i + 1]}\")\n",
    "        \n",
    "        # Look for Final Answer\n",
    "        if 'Final Answer:' in context:\n",
    "            answer_start = context.find('Final Answer:') + 13\n",
    "            final_answer = context[answer_start:].strip()\n",
    "            print(f\"‚úÖ Final Answer: {final_answer}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚ùå Actual answer given: {item['reasoning']['response']}\")\n",
    "print(f\"üìä Score: {item['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65fcb7-9ad5-4258-8723-85e61285879a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; border: 1px solid #ddd; padding: 15px; border-radius: 5px; margin: 15px 0; font-family: monospace;\">\n",
    "<strong>Agent's Reasoning Trace:</strong>\n",
    "<pre style=\"margin: 10px 0; white-space: pre-wrap;\">\n",
    "ü§ñ AGENT'S DECISION PROCESS\n",
    "============================================================\n",
    "Question: What was Austria's average temperature in 1980?\n",
    "Expected: 6.80¬∞C\n",
    "============================================================\n",
    "Step 1\n",
    "üí≠ Thought: I need to get temperature data for Austria in 1980\n",
    "üõ†Ô∏è  Tool: calculate_statistics\n",
    "üì• Input: {\"country\": \"Austria\", \"start_year\": 1980, \"end_year\": 1980}\n",
    "üì§ Response: {\"mean_temperature\": 8.08, \"years_analyzed\": \"1950-2025\", ...}\n",
    "‚úÖ Final Answer: 8.08¬∞C\n",
    "============================================================\n",
    "‚ùå Actual answer given: 8.08¬∞C\n",
    "üìä Score: 0.0\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17c8e2-7545-4e1c-b470-5678d57e8965",
   "metadata": {},
   "source": [
    "Looking at Step 1, you can see that the agent passed the correct country, but failed to provide the year 1980. In the following steps, the agent tried to work around the incomplete input by calculating the average temperature for Austria across all the years it had data for, coming up with the wrong answer. \n",
    "\n",
    "The entire output can be found at `climate_analyzer/.tmp/nat/climate_analyzer/eval/simple_test/answer_accuracy_output.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b98fb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Bug Discovery!\n",
    "<div style=\"background-color: #ffebee; border: 2px solid #f44336; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "<h3 style=\"color: #c62828; margin-top: 0;\">üêõ Critical Bug Identified</h3>\n",
    "<div style=\"background-color: white; padding: 15px; border-radius: 5px; margin: 15px 0;\">\n",
    "<h4 style=\"color: #f44336; margin-top: 0;\">The Problem</h4>\n",
    "<strong>The tool ignores year parameters!</strong>\n",
    "<br><br>\n",
    "<table style=\"width: 100%; border-collapse: collapse;\">\n",
    "    <tr style=\"background-color: #e8f5e9;\">\n",
    "        <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>‚úÖ Agent Did Right</strong></td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #ddd;\">Passed correct parameters: <code>country='Austria', start_year=1980, end_year=1980</code></td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #ffebee;\">\n",
    "        <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>‚ùå Tool Did Wrong</strong></td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #ddd;\">Returned data for ALL years (1950-2025), not just 1980</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #fff3cd;\">\n",
    "        <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>üìä Result</strong></td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #ddd;\">Wrong answer: 8.08¬∞C (average across 75 years) instead of 6.80¬∞C (1980 only)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "</div>\n",
    "<div style=\"background-color: white; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
    "<h4 style=\"color: #f44336; margin-top: 0;\">Root Cause</h4>\n",
    "The <code>calculate_statistics</code> function accepts <code>start_year</code> and <code>end_year</code> parameters but doesn't actually filter the data by them. The function signature has the parameters, but the implementation doesn't use them.\n",
    "<br><br>\n",
    "<strong>This is why systematic evaluation matters</strong> - manual testing might never catch this edge case, but automated evaluation found it immediately.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "## The Fix\n",
    "Add year filtering logic to the calculate_statistics function:\n",
    "<div style=\"background-color: #e8f5e9; border-left: 6px solid #4CAF50; padding: 15px; margin: 20px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">‚úÖ The Solution</h4>\n",
    "<strong>Before (broken code):</strong>\n",
    "<pre style=\"background-color: white; padding: 10px; border-radius: 3px; margin: 10px 0;\">\n",
    "def calculate_statistics(df, country=None, start_year=None, end_year=None):\n",
    "    # Filter by country\n",
    "    if country:\n",
    "        df = df[df['country_name'] == country] # BUG: Never filters by year!\n",
    "    return calculate_stats(df)\n",
    "</pre>\n",
    "<strong>After (fixed code):</strong>\n",
    "<pre style=\"background-color: white; padding: 10px; border-radius: 3px; margin: 10px 0;\">\n",
    "def calculate_statistics(df, country=None, start_year=None, end_year=None):\n",
    "    # Filter by country\n",
    "    if country:\n",
    "        df = df[df['country_name'] == country]\n",
    "        if start_year is not None: # ‚úÖ FIX: Actually filter by year when specified\n",
    "            df = df[df['year'] >= start_year]\n",
    "        if end_year is not None:\n",
    "            df = df[df['year'] <= end_year]\n",
    "        return calculate_stats(df)\n",
    "</pre>\n",
    "</div>\n",
    "<div style=\"background-color: #f3e5f5; border-left: 6px solid #9c27b0; padding: 15px; margin: 15px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">üí° Why This Bug Existed</h4>\n",
    "<ul>\n",
    "<li><strong>Interface vs. Implementation</strong> - The function signature promised year filtering, but the body didn't deliver</li>\n",
    "<li><strong>Silent failure</strong> - No error was thrown; the function just returned wrong data</li>\n",
    "<li><strong>Hard to catch manually</strong> - \"Show me Austria's temperature\" works fine. Only specific year queries fail.</li>\n",
    "<li><strong>Evaluation caught it</strong> - Systematic testing with ground truth revealed the bug immediately</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525807a",
   "metadata": {},
   "source": [
    "## Test the Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c68fdc7",
   "metadata": {
    "height": 62
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-17 20:12:09 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: src/climate_analyzer/configs/eval_config_fixed.yml\n",
      "2025-12-17 20:12:11 - WARNING  - nat.builder.function_info:455 - Using provided input_schema for multi-argument function\n",
      "2025-12-17 20:12:11 - WARNING  - nat.builder.function_info:455 - Using provided input_schema for multi-argument function\n",
      "2025-12-17 20:12:11 - WARNING  - nat.builder.function_info:455 - Using provided input_schema for multi-argument function\n",
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:171: UserWarning: http://jupyter-api-proxy.internal.dlai/rev-proxy/nvidia does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:171: UserWarning: http://jupyter-api-proxy.internal.dlai/rev-proxy/nvidia does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:171: UserWarning: http://jupyter-api-proxy.internal.dlai/rev-proxy/nvidia does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n",
      "Running workflow: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:13<00:00, 13.97s/it]\n",
      "Evaluating Ragas nv_accuracy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.57s/it]\n",
      "2025-12-17 20:12:28 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-12-17 20:12:28 - INFO     - nat.eval.evaluate:337 - Workflow output written to .tmp/nat/climate_analyzer/eval/fixed_test/workflow_output.json\n",
      "2025-12-17 20:12:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to .tmp/nat/climate_analyzer/eval/fixed_test/answer_accuracy_output.json\n",
      "2025-12-17 20:12:28 - INFO     - nat.eval.utils.output_uploader:61 - No S3 config provided; skipping upload.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run evaluation with the fixed tool\n",
    "!cd climate_analyzer && nat eval --config_file src/climate_analyzer/configs/eval_config_fixed.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdacca",
   "metadata": {},
   "source": [
    "## Verify Results\n",
    "Now that the logic has been updated, check the results again to see if the score improved: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32e38d1d",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('climate_analyzer/.tmp/nat/climate_analyzer/eval/fixed_test/answer_accuracy_output.json', 'r') as f:\n",
    "    answer_accuracy_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4635b9",
   "metadata": {
    "height": 232
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Results\n",
      "==================================================\n",
      "Average Score: 1.0 / 1.0\n",
      "\n",
      "‚ùì What was the average temperature in Austria in 1980? Please provide the numerical value.\n",
      "‚úÖ Expected: The average temperature in Austria in 1980 was 6.80¬∞C\n",
      "‚ùå Got: The average temperature in Austria in 1980 was 6.8¬∞C.\n",
      "üìà Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìä Evaluation Results\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Average Score: {answer_accuracy_data['average_score']} / 1.0\")\n",
    "print()\n",
    "\n",
    "for item in answer_accuracy_data['eval_output_items']:\n",
    "    r = item['reasoning']\n",
    "    print(f\"‚ùì {r['user_input']}\")\n",
    "    print(f\"‚úÖ Expected: {r['reference']}\")\n",
    "    print(f\"‚ùå Got: {r['response']}\")\n",
    "    print(f\"üìà Score: {item['score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa1ccf-f833-44d6-b4fc-55273c50a6cf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f5e9; border-left: 6px solid #4CAF50; padding: 15px; margin: 20px 0;\">\n",
    "<h4 style=\"margin-top: 0;\">‚úÖ Fixed Results - Success!</h4>\n",
    "<pre style=\"background-color: white; padding: 10px; border-radius: 3px; margin: 10px 0;\">\n",
    "üìä Evaluation Results\n",
    "==================================================\n",
    "Average Score: 1.0 / 1.0\n",
    "‚ùì What was Austria's average temperature in 1980?\n",
    "‚úÖ Expected: 6.80¬∞C\n",
    "‚úÖ Got: 6.80¬∞C\n",
    "üìà Score: 1.0\n",
    "</pre>\n",
    "<br>\n",
    "<strong>Perfect score!</strong> The agent now correctly filters by year and returns accurate results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e31398",
   "metadata": {},
   "source": [
    "## Summary\n",
    "<div style=\"background-color: #e3f2fd; border: 2px solid #2196F3; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "<h3 style=\"color: #1976d2; margin-top: 0;\">üéâ What You Accomplished</h3>\n",
    "<div style=\"background-color: white; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
    "<h4 style=\"color: #4CAF50; margin-top: 0;\">‚úÖ The Evaluation ‚Üí Fix ‚Üí Verify Loop</h4>\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center; margin: 15px 0; flex-wrap: wrap;\">\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <div style=\"background-color: #2196F3; color: white; padding: 12px; border-radius: 8px;\">\n",
    "            <strong>1. Create Tests</strong>\n",
    "        </div>\n",
    "        <small>Ground truth dataset</small>\n",
    "    </div>\n",
    "    <div style=\"font-size: 20px;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <div style=\"background-color: #ff9800; color: white; padding: 12px; border-radius: 8px;\">\n",
    "            <strong>2. Run Evaluation</strong>\n",
    "        </div>\n",
    "        <small>Found score: 0.0</small>\n",
    "    </div>\n",
    "    <div style=\"font-size: 20px;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <div style=\"background-color: #f44336; color: white; padding: 12px; border-radius: 8px;\">\n",
    "            <strong>3. Discover Bug</strong>\n",
    "        </div>\n",
    "        <small>Year filter missing</small>\n",
    "    </div>\n",
    "    <div style=\"font-size: 20px;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <div style=\"background-color: #9C27B0; color: white; padding: 12px; border-radius: 8px;\">\n",
    "            <strong>4. Fix Code</strong>\n",
    "        </div>\n",
    "        <small>Add year filtering</small>\n",
    "    </div>\n",
    "    <div style=\"font-size: 20px;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <div style=\"background-color: #4CAF50; color: white; padding: 12px; border-radius: 8px;\">\n",
    "            <strong>5. Verify Fix</strong>\n",
    "        </div>\n",
    "        <small>Score: 1.0 ‚úÖ</small>\n",
    "    </div>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"background-color: white; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
    "<h4 style=\"color: #2196F3; margin-top: 0;\">üìä Before vs. After</h4>\n",
    "<table style=\"width: 100%; border-collapse: collapse; margin-top: 10px;\">\n",
    "    <tr style=\"background-color: #2196F3; color: white;\">\n",
    "        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Metric</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #ddd;\">Before Fix</th>\n",
    "        <th style=\"padding: 12px; text-align: center; border: 1px solid #ddd;\">After Fix</th>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: white;\">\n",
    "        <td style=\"padding: 12px; border: 1px solid #ddd;\"><strong>Evaluation Score</strong></td>\n",
    "        <td style=\"padding: 12px; text-align: center; border: 1px solid #ddd; color: #f44336;\"><strong>0.0 / 1.0</strong></td>\n",
    "        <td style=\"padding: 12px; text-align: center; border: 1px solid #ddd; color: #4CAF50;\"><strong>1.0 / 1.0</strong></td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #f9f9f9;\">\n",
    "        <td style=\"padding: 12px; border: 1px solid #ddd;\"><strong>Answer for 1980</strong></td>\n",
    "        <td style=\"padding: 12px; text-align: center; border: 1px solid #ddd; color: #f44336;\">8.08¬∞C (wrong)</td>\n",
    "        <td style=\"padding: 12px; text-align: center; border: 1px solid #ddd; color: #4CAF50;\">6.80¬∞C (correct)</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: white;\">\n",
    "        <td style=\"padding: 12px; border: 1px solid #ddd;\"><strong>Year Filtering</strong></td>\n",
    "        <td style=\"padding: 12px; text-align: center; border: 1px solid #ddd; color: #f44336;\">‚ùå Broken</td>\n",
    "        <td style=\"padding: 12px; text-align: center; border: 1px solid #ddd; color: #4CAF50;\">‚úÖ Working</td>\n",
    "    </tr>\n",
    "</table>\n",
    "</div>\n",
    "<div style=\"background-color: white; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
    "<h4 style=\"color: #9C27B0; margin-top: 0;\">üîë Key Insights</h4>\n",
    "<ul>\n",
    "<li><strong>Evaluation finds silent bugs</strong> - No errors thrown, just wrong answers</li>\n",
    "<li><strong>Ground truth is essential</strong> - You need known-correct answers to test against</li>\n",
    "<li><strong>Systematic beats manual</strong> - Automated evaluation catches edge cases you'd miss</li>\n",
    "<li><strong>Reasoning traces debug bugs</strong> - Seeing what the agent tried helps identify where it went wrong</li>\n",
    "<li><strong>Verify fixes work</strong> - Re-run evaluation to confirm the bug is actually fixed</li>\n",
    "</ul>\n",
    "</div>\n",
    "<div style=\"background-color: #fff3cd; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
    "<h4 style=\"margin-top: 0;\">‚ö° Why This Matters in Production</h4>\n",
    "Without systematic evaluation:\n",
    "<ul>\n",
    "<li>This bug would have made it to production</li>\n",
    "<li>Users asking about specific years would get wrong answers</li>\n",
    "<li>You'd only discover it through user complaints</li>\n",
    "<li>You wouldn't know how widespread the problem is</li>\n",
    "</ul>\n",
    "<br>\n",
    "With evaluation:\n",
    "<ul>\n",
    "<li>Caught the bug before deployment</li>\n",
    "<li>Fixed it with confidence (verified the fix works)</li>\n",
    "<li>Can now test regression (make sure future changes don't break it again)</li>\n",
    "<li>Have metrics to track improvements over time</li>\n",
    "</ul>\n",
    "</div>\n",
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
    "<h4 style=\"margin-top: 0;\">üöÄ Next Lesson: Deploy with UI</h4>\n",
    "Your agent is now:\n",
    "<ul>\n",
    "<li>‚úÖ Functional (has data analysis tools)</li>\n",
    "<li>‚úÖ Observable (Phoenix tracing shows decisions)</li>\n",
    "<li>‚úÖ Enhanced (LangGraph calculator for complex math)</li>\n",
    "<li>‚úÖ Tested (evaluation ensures correctness)</li>\n",
    "</ul>\n",
    "<br>\n",
    "In the final lesson, you'll:\n",
    "<ul>\n",
    "<li>Deploy your agent with a production-ready UI</li>\n",
    "<li>Add authentication and rate limiting</li>\n",
    "<li>See how everything comes together in a real application</li>\n",
    "<li>Share your agent with users</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
